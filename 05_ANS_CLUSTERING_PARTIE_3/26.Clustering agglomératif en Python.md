# Clustering agglomératif en Python

### Introduction
Nous allons explorer comment réaliser un clustering hiérarchique en utilisant Scikit-Learn. Bien que SciPy soit utile pour visualiser les dendrogrammes, Scikit-Learn est la bibliothèque la plus couramment utilisée pour la modélisation machine learning en Python. La cohérence de la syntaxe entre les différents modèles de Scikit-Learn facilite l'apprentissage et l'utilisation.

### Utilisation de Scikit-Learn pour le Clustering Hiérarchique

#### Étape 1 : Importation des Bibliothèques
Nous commençons par importer les bibliothèques nécessaires.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering

# Générer des données synthétiques
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=0)

# Afficher les données
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title("Données Synthétiques")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

#### Étape 2 : Application du Clustering Hiérarchique Agglomératif
Nous utilisons la classe `AgglomerativeClustering` de Scikit-Learn pour réaliser le clustering.

```python
# Instanciation du modèle de clustering agglomératif
model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')

# Ajustement du modèle aux données
model.fit(X)

# Affichage des étiquettes de cluster
labels = model.labels_
print(labels)

# Visualisation des clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)
plt.title("Clustering Hiérarchique avec Scikit-Learn")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### Explication des Paramètres

- **n_clusters** : Nombre de clusters à former. Ici, nous avons spécifié trois clusters en nous basant sur notre interprétation visuelle du dendrogramme.
- **affinity** : Métrique de distance utilisée pour calculer la distance entre les points de données. La valeur par défaut est `euclidean`.
- **linkage** : Méthode utilisée pour déterminer la distance entre les clusters. La valeur par défaut est `ward`, qui minimise l'augmentation de la variance totale.

### Détails des Arguments
- **n_clusters** : La valeur par défaut est 2. Nous pouvons ajuster ce paramètre en fonction de notre interprétation visuelle du dendrogramme.
- **metric** : La valeur par défaut est `euclidean`, mais nous pouvons utiliser d'autres métriques telles que `manhattan`, `cosine`, ou `precomputed` pour fournir nos propres distances.
- **linkage** : La valeur par défaut est `ward`, mais nous pouvons également utiliser `single`, `complete`, ou `average`.

### Visualisation et Analyse des Résultats
Pour visualiser les clusters, nous utilisons un scatter plot en coloriant chaque point de données en fonction de son étiquette de cluster.

```python
# Visualisation des clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)
plt.title("Clustering Hiérarchique avec Scikit-Learn")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### Conclusion
Le clustering hiérarchique agglomératif est une méthode puissante pour segmenter les données. Scikit-Learn simplifie l'application de cette technique avec une syntaxe cohérente et des paramètres ajustables. Vous pouvez explorer différents paramètres et métriques de distance pour adapter le modèle à vos données spécifiques.

Dans la prochaine section, nous allons approfondir les détails sur les différentes métriques de distance et les méthodes de liaison, et comment elles peuvent influencer les résultats du clustering hiérarchique.
