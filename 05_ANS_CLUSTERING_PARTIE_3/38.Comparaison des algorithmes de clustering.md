### 98. Comparaison des algorithmes de clustering

Dans cette section, nous allons comparer différents algorithmes de clustering que nous avons appliqués jusqu'à présent. Nous examinerons K-means, le clustering hiérarchique (agglomératif), et DBscan. Chaque méthode a ses propres avantages et inconvénients, et l'objectif est de comprendre comment ces algorithmes se comportent sur différents ensembles de données.

#### K-means Clustering

- **Avantages**:
  - Simple à comprendre et à implémenter.
  - Rapide pour des grands ensembles de données.
  - Fonctionne bien si les clusters sont globulaires et bien séparés.

- **Inconvénients**:
  - Doit spécifier le nombre de clusters à l'avance.
  - Sensible aux valeurs aberrantes et au bruit.
  - Fonctionne mal pour des clusters de formes irrégulières.

#### Clustering Hiérarchique (Agglomératif)

- **Avantages**:
  - Ne nécessite pas de spécifier le nombre de clusters à l'avance.
  - Génère un dendrogramme, permettant une visualisation des relations entre les points.
  - Peut capturer des clusters de formes variées.

- **Inconvénients**:
  - Plus lent et inefficace pour des grands ensembles de données.
  - Nécessite des décisions sur les critères de liaison (simple, complet, moyen, etc.).
  - Sensible au bruit et aux valeurs aberrantes.

#### DBscan (Density-Based Spatial Clustering of Applications with Noise)

- **Avantages**:
  - Identifie des clusters de formes arbitraires.
  - Capable de gérer des valeurs aberrantes et du bruit.
  - Ne nécessite pas de spécifier le nombre de clusters à l'avance.

- **Inconvénients**:
  - Les résultats dépendent fortement des paramètres epsilon et min_samples.
  - Moins performant pour des clusters de densité très variable.
  - Peut être difficile à utiliser avec des ensembles de données de haute dimension.

### Exemple de Comparaison sur un Jeu de Données de Céréales

Pour illustrer ces points, nous avons appliqué ces trois algorithmes sur un jeu de données de céréales. Voici les étapes détaillées pour chaque méthode et les résultats obtenus.

#### Étape 1: Importer les Bibliothèques Nécessaires

```python
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt
```

#### Étape 2: Charger et Préparer le Jeu de Données

```python
# Charger le jeu de données des céréales
data = pd.read_csv('cereal.csv')

# Sélectionner les caractéristiques numériques pertinentes
numeric_data = data[['Calories', 'Protein', 'Sodium', 'Fiber']]

# Normaliser les données
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)
```

#### Étape 3: Appliquer K-means

```python
# K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(scaled_data)

# Score de silhouette pour K-means
kmeans_silhouette = silhouette_score(scaled_data, kmeans_labels)
print(f"Score de silhouette pour K-means: {kmeans_silhouette}")
```

#### Étape 4: Appliquer Clustering Hiérarchique (Agglomératif)

```python
# Clustering hiérarchique agglomératif
agglo = AgglomerativeClustering(n_clusters=3)
agglo_labels = agglo.fit_predict(scaled_data)

# Score de silhouette pour le clustering hiérarchique
agglo_silhouette = silhouette_score(scaled_data, agglo_labels)
print(f"Score de silhouette pour le clustering hiérarchique: {agglo_silhouette}")
```

#### Étape 5: Appliquer DBscan

```python
def tune_dbscan(data, eps_values, min_samples_values):
    results = []
    
    for eps in eps_values:
        for min_samples in min_samples_values:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            dbscan.fit(data)
            labels = dbscan.labels_
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            
            if n_clusters > 1:
                score = silhouette_score(data, labels)
            else:
                score = -1
            
            results.append((eps, min_samples, n_clusters, n_noise, score))
    
    results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters', 'n_noise', 'silhouette_score'])
    return results_df

# Définir les plages de valeurs pour epsilon et min_samples
eps_values = np.arange(0.1, 2.1, 0.1)
min_samples_values = range(2, 11)

# Appliquer la fonction aux données original et normalisées
dbscan_results_original = tune_dbscan(numeric_data, eps_values, min_samples_values)
dbscan_results_scaled = tune_dbscan(scaled_data, eps_values, min_samples_values)

# Trouver les meilleures combinaisons de paramètres
best_original = dbscan_results_original.sort_values(by='silhouette_score', ascending=False).iloc[0]
best_scaled = dbscan_results_scaled.sort_values(by='silhouette_score', ascending=False).iloc[0]

print(f"Meilleure combinaison pour les données originales: eps={best_original['eps']}, min_samples={best_original['min_samples']}")
print(f"Meilleure combinaison pour les données normalisées: eps={best_scaled['eps']}, min_samples={best_scaled['min_samples']}")
```

#### Étape 6: Comparer les Résultats

```python
# Résultats pour K-means
print(f"Score de silhouette pour K-means: {kmeans_silhouette}")

# Résultats pour le clustering hiérarchique
print(f"Score de silhouette pour le clustering hiérarchique: {agglo_silhouette}")

# Résultats pour DBscan
print(f"Meilleure combinaison pour DBscan sur les données normalisées: eps={best_scaled['eps']}, min_samples={best_scaled['min_samples']}")
print(f"Score de silhouette pour le meilleur modèle DBscan: {best_scaled['silhouette_score']}")
```

### Conclusion

En utilisant ces différentes méthodes de clustering, nous avons pu voir comment chaque algorithme se comporte avec le jeu de données des céréales. En comparant les scores de silhouette, nous pouvons déterminer quel algorithme a produit les clusters les plus distincts et les plus appropriés pour ce jeu de données spécifique. DBscan, avec ses capacités à détecter les formes de clusters irrégulières et à identifier les points de bruit, a montré des résultats prometteurs lorsqu'il est correctement ajusté.
