
Pour cette dernière mission sur DBscan, nous allons suivre les étapes ci-dessous pour créer des modèles DBscan sur les ensembles de données originaux et standardisés, en utilisant différentes valeurs pour epsilon et min_samples afin de trouver les meilleures combinaisons basées sur le score de silhouette.

### Étape 1 : Importer les Bibliothèques Nécessaires

```python
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
```

### Étape 2 : Charger et Préparer le Jeu de Données

```python
# Charger le jeu de données des céréales
data = pd.read_csv('cereal.csv')

# Sélectionner les caractéristiques numériques pertinentes
numeric_data = data[['Calories', 'Protein', 'Sodium', 'Fiber']]

# Normaliser les données
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)
```

### Étape 3 : Définir la Fonction pour Tester les Paramètres de DBSCAN

```python
def tune_dbscan(data, eps_values, min_samples_values):
    results = []
    
    for eps in eps_values:
        for min_samples in min_samples_values:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            dbscan.fit(data)
            labels = dbscan.labels_
            
            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)
            
            if n_clusters > 1:
                score = silhouette_score(data, labels)
            else:
                score = -1
            
            results.append((eps, min_samples, n_clusters, n_noise, score))
    
    results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters', 'n_noise', 'silhouette_score'])
    return results_df
```

### Étape 4 : Définir les Plages de Valeurs pour epsilon et min_samples

```python
eps_values = np.arange(0.1, 2.1, 0.1)
min_samples_values = range(2, 11)
```

### Étape 5 : Appliquer la Fonction aux Données Originales et Normalisées

```python
# Tester sur les données originales
original_results = tune_dbscan(numeric_data, eps_values, min_samples_values)
print("Meilleurs résultats pour les données originales:")
print(original_results.sort_values(by='silhouette_score', ascending=False).head())

# Tester sur les données normalisées
scaled_results = tune_dbscan(scaled_data, eps_values, min_samples_values)
print("Meilleurs résultats pour les données normalisées:")
print(scaled_results.sort_values(by='silhouette_score', ascending=False).head())
```

### Étape 6 : Trouver les Meilleures Combinaisons de Paramètres

```python
# Trouver les meilleures combinaisons pour les données originales
best_original = original_results.sort_values(by='silhouette_score', ascending=False).iloc[0]
print(f"Meilleure combinaison pour les données originales: eps={best_original['eps']}, min_samples={best_original['min_samples']}")

# Trouver les meilleures combinaisons pour les données normalisées
best_scaled = scaled_results.sort_values(by='silhouette_score', ascending=False).iloc[0]
print(f"Meilleure combinaison pour les données normalisées: eps={best_scaled['eps']}, min_samples={best_scaled['min_samples']}")
```

### Étape 7 : Appliquer le Meilleur Modèle DBSCAN aux Données

```python
# Meilleur modèle pour les données originales
dbscan_best_original = DBSCAN(eps=best_original['eps'], min_samples=best_original['min_samples'])
dbscan_best_original.fit(numeric_data)
labels_best_original = dbscan_best_original.labels_
data['Best_Cluster_Original'] = labels_best_original

# Meilleur modèle pour les données normalisées
dbscan_best_scaled = DBSCAN(eps=best_scaled['eps'], min_samples=best_scaled['min_samples'])
dbscan_best_scaled.fit(scaled_data)
labels_best_scaled = dbscan_best_scaled.labels_
data['Best_Cluster_Scaled'] = labels_best_scaled

# Afficher les étiquettes de clusters
print("Étiquettes de clusters pour les données originales :")
print(data['Best_Cluster_Original'].value_counts())
print("Étiquettes de clusters pour les données normalisées :")
print(data['Best_Cluster_Scaled'].value_counts())
```

### Résumé

1. Nous avons importé les bibliothèques nécessaires.
2. Nous avons chargé et préparé le jeu de données.
3. Nous avons défini une fonction pour tester plusieurs valeurs de `eps` et `min_samples` pour DBSCAN.
4. Nous avons appliqué cette fonction aux ensembles de données originaux et normalisés.
5. Nous avons identifié les meilleures combinaisons de `eps` et `min_samples` basées sur le score de silhouette.
6. Nous avons appliqué les meilleurs modèles DBSCAN aux ensembles de données et examiné les étiquettes de clusters.

Avec ces étapes, vous pourrez trouver les meilleures combinaisons de paramètres pour DBSCAN et évaluer les résultats en utilisant le score de silhouette.
