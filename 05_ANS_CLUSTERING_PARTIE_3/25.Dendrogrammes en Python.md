# Dendrogrammes en Python

### Introduction
Dans cette section, nous allons explorer deux façons de réaliser un clustering hiérarchique en Python. La première méthode utilise la fonction `dendrogram` de la bibliothèque SciPy. SciPy est une bibliothèque scientifique pour Python qui permet de réaliser de nombreux calculs scientifiques, y compris la visualisation des clusters hiérarchiques.

### Utilisation de SciPy pour le Clustering Hiérarchique

#### Étape 1 : Importation des Bibliothèques
Commencez par importer les bibliothèques nécessaires. Nous allons utiliser `linkage` et `dendrogram` de SciPy, ainsi que `matplotlib` pour la visualisation.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import linkage, dendrogram

# Générer des données synthétiques
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=0)

# Afficher les données
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title("Données Synthétiques")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

#### Étape 2 : Calcul des Liens Hiérarchiques
Utilisez la fonction `linkage` pour calculer les distances entre tous les points de données et créer la matrice de liaison.

```python
# Calculer les liens hiérarchiques en utilisant la méthode de Ward
Z = linkage(X, method='ward')

# Afficher la matrice de liaison
print(Z)
```

#### Étape 3 : Création du Dendrogramme
Utilisez la fonction `dendrogram` pour créer et visualiser le dendrogramme.

```python
# Créer le dendrogramme
plt.figure(figsize=(10, 7))
dendrogram(Z)
plt.title("Dendrogramme")
plt.xlabel("Points de Données")
plt.ylabel("Distance Euclidienne")
plt.show()
```

#### Étape 4 : Ajustement des Seuils de Couleur
Pour mieux interpréter le dendrogramme, ajustez le seuil de couleur pour visualiser un certain nombre de clusters.

```python
# Ajuster le seuil de couleur pour afficher trois clusters
plt.figure(figsize=(10, 7))
dendrogram(Z, color_threshold=10)
plt.title("Dendrogramme avec Seuil de Couleur")
plt.xlabel("Points de Données")
plt.ylabel("Distance Euclidienne")
plt.show()
```

### Interprétation du Dendrogramme
Le dendrogramme montre les fusions successives de clusters et la distance à laquelle chaque fusion se produit. En ajustant le seuil de couleur, vous pouvez visualiser les différents clusters. Par exemple, avec un seuil de 10, nous pouvons voir clairement trois clusters distincts.

### Utilisation de Scikit-Learn pour le Clustering Hiérarchique

#### Étape 1 : Importation des Bibliothèques
Nous allons maintenant utiliser `scikit-learn` pour effectuer le même clustering hiérarchique.

```python
from sklearn.cluster import AgglomerativeClustering

# Appliquer le clustering hiérarchique agglomératif
model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')
labels = model.fit_predict(X)

# Afficher les clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)
plt.title("Clustering Hiérarchique avec Scikit-Learn")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### Conclusion
Le clustering hiérarchique est une technique puissante pour explorer les relations entre les points de données. En utilisant SciPy, vous pouvez créer des dendrogrammes pour visualiser ces relations et ajuster les seuils pour mieux comprendre la structure des clusters. Avec Scikit-Learn, vous pouvez appliquer facilement des modèles de clustering hiérarchique pour segmenter vos données.

Dans la prochaine section, nous explorerons en détail les distances Euclidienne et Manhattan, et comment elles peuvent influencer les résultats de clustering hiérarchique.
