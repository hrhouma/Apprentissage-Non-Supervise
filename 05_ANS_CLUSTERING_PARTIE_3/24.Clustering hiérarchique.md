## Clustering hiérarchique : Un cours exhaustif

### Introduction au Clustering Hiérarchique
Le clustering hiérarchique est une méthode de regroupement de données qui crée une hiérarchie de clusters. Contrairement au K-means clustering, qui se concentre sur les centroïdes des clusters, le clustering hiérarchique regroupe les points de données similaires en formant des clusters imbriqués.

### Concepts de Base
#### Visualisation
Prenons un exemple visuel. Considérons un nuage de points avec six points distincts. À la fin du processus de clustering hiérarchique, nous avons identifié deux clusters distincts. Le processus commence par calculer les distances entre toutes les paires de points de données, et le résultat est souvent visualisé sous forme d'un dendrogramme.

#### Dendrogramme
Un dendrogramme est un diagramme arborescent qui montre les relations de similarité entre les points de données. Chaque branche du dendrogramme représente une fusion de clusters, et la hauteur des branches indique la distance ou la dissimilarité entre les clusters fusionnés.

### Étapes du Clustering Hiérarchique
Voici les étapes principales du clustering hiérarchique :

1. **Calcul des distances** : Calculer les distances entre toutes les paires de points de données.
2. **Regroupement initial** : Trouver les deux points les plus proches et les regrouper en un cluster.
3. **Fusion progressive** : Trouver les clusters ou points les plus proches restants et les regrouper. Répéter ce processus jusqu'à ce qu'il ne reste qu'un seul cluster englobant tous les points.

### Types de Clustering Hiérarchique
#### Clustering Agglomératif (Bottom-up)
- **Processus** : Commencer avec chaque point de données comme un cluster individuel. Fusionner les clusters les plus proches jusqu'à ce qu'il ne reste qu'un seul cluster.
- **Avantages** : Simple à comprendre et à implémenter.
- **Inconvénients** : Peut être sensible aux erreurs initiales de clustering.

#### Clustering Divisif (Top-down)
- **Processus** : Commencer avec un seul cluster englobant tous les points de données. Diviser progressivement ce cluster en sous-clusters jusqu'à ce que chaque point soit un cluster individuel.
- **Avantages** : Peut mieux gérer les outliers.
- **Inconvénients** : Moins couramment utilisé en pratique en raison de sa complexité.

### Méthodes de Calcul de Distance
#### Distance Euclidienne
- **Description** : Distance en ligne droite entre deux points.
- **Utilisation** : La plus courante, surtout lorsque les données sont sur la même échelle.

#### Distance de Manhattan
- **Description** : Distance en termes de blocs de ville, seulement horizontalement et verticalement.
- **Utilisation** : Utile lorsque les dimensions ont des échelles différentes ou des outliers.

#### Distance Cosine
- **Description** : Mesure l'angle entre deux vecteurs de points de données.
- **Utilisation** : Fréquemment utilisée pour les données textuelles et les systèmes de recommandation.

### Méthodes de Liaison (Linkage Methods)
#### Liaison Simple (Single Linkage)
- **Description** : Distance entre les points les plus proches de deux clusters.
- **Avantages** : Simple et intuitif.
- **Inconvénients** : Peut créer des chaînes de points, conduisant à des clusters allongés et peu naturels.

#### Liaison Complète (Complete Linkage)
- **Description** : Distance entre les points les plus éloignés de deux clusters.
- **Avantages** : Tend à créer des clusters compacts et sphériques.
- **Inconvénients** : Peut être influencée par des outliers.

#### Liaison Moyenne (Average Linkage)
- **Description** : Moyenne des distances entre tous les points de deux clusters.
- **Avantages** : Compromis entre la liaison simple et la liaison complète.
- **Inconvénients** : Plus complexe à calculer.

#### Méthode de Ward (Ward's Method)
- **Description** : Minimise l'augmentation de la variance totale à chaque fusion de clusters.
- **Avantages** : Crée des clusters compacts et bien séparés.
- **Inconvénients** : Calculs plus intensifs.

### Exemple Pratique avec Python
Pour illustrer ces concepts, utilisons Python et la bibliothèque `scikit-learn` pour effectuer un clustering hiérarchique sur un ensemble de données simple.

#### Chargement des Données
```python
import pandas as pd
import numpy as np
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt

# Générer des données synthétiques
X, y = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=0)

# Visualiser les données
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.show()
```

#### Clustering Hiérarchique avec Scikit-Learn
```python
from scipy.cluster.hierarchy import dendrogram, linkage

# Calculer les liens hiérarchiques
Z = linkage(X, 'ward')

# Tracer le dendrogramme
plt.figure(figsize=(10, 7))
dendrogram(Z, labels=y)
plt.show()
```

#### Interprétation du Dendrogramme
Le dendrogramme résultant montre les fusions successives de clusters et la distance à laquelle chaque fusion se produit. Vous pouvez choisir un seuil pour couper le dendrogramme et déterminer le nombre optimal de clusters.

### Conclusion
Le clustering hiérarchique est une technique puissante et flexible pour explorer et analyser les données. En comprenant les différentes méthodes de distance et de liaison, ainsi que les types de clustering hiérarchique, vous pouvez mieux segmenter vos données et en tirer des insights significatifs.

Ce cours exhaustif vous a fourni une compréhension approfondie du clustering hiérarchique, des concepts de base aux applications pratiques avec Python. N'hésitez pas à expérimenter avec différents ensembles de données et méthodes pour approfondir votre compréhension.
