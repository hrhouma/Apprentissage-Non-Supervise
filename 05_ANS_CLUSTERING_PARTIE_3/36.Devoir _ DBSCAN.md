
## Démo : Utiliser DBSCAN et Score de Silhouette Ensemble pour Trouver le Meilleur Modèle DBSCAN

Dans cette démo, nous allons voir comment utiliser DBSCAN et le score de silhouette ensemble pour trouver le meilleur modèle DBSCAN.

### Étape 1 : Importer les Bibliothèques Nécessaires

Tout d'abord, nous allons importer les bibliothèques nécessaires :

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
```

### Étape 2 : Charger et Préparer le Jeu de Données

Chargeons le jeu de données et sélectionnons les caractéristiques numériques pertinentes. Dans ce cas, nous utiliserons le jeu de données des céréales :

```python
# Charger le jeu de données des céréales
data = pd.read_csv('cereal.csv')

# Sélectionner les caractéristiques numériques pertinentes
numeric_data = data[['Calories', 'Protein', 'Sodium', 'Fiber']]
```

### Étape 3 : Normaliser les Données

Normalisons les données en utilisant `StandardScaler` pour nous assurer que toutes les caractéristiques contribuent également au processus de clustering :

```python
# Normaliser les données
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)
```

### Étape 4 : Appliquer DBSCAN

Appliquons l'algorithme de clustering DBSCAN aux données normalisées :

```python
# Appliquer DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(scaled_data)
labels = dbscan.labels_

# Ajouter les étiquettes de clusters aux données originales
data['Cluster'] = labels
```

### Étape 5 : Calculer le Score de Silhouette

Calculons le score de silhouette pour le modèle DBSCAN afin d'évaluer la qualité du clustering :

```python
# Calculer le score de silhouette
score = silhouette_score(scaled_data, labels)
print(f"Score de Silhouette : {score}")
```

### Étape 6 : Ajuster les Paramètres de DBSCAN et Recalculer le Score de Silhouette

Si le score de silhouette initial n'est pas satisfaisant, ajustons les paramètres `eps` et `min_samples` de DBSCAN et recalculons le score de silhouette :

```python
# Ajuster les paramètres de DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=10)
dbscan.fit(scaled_data)
labels = dbscan.labels_

# Ajouter les nouvelles étiquettes de clusters aux données originales
data['Cluster'] = labels

# Recalculer le score de silhouette
score = silhouette_score(scaled_data, labels)
print(f"Score de Silhouette avec eps=0.3 et min_samples=10 : {score}")
```

### Étape 7 : Boucle pour Tester Différents Paramètres de DBSCAN

Pour trouver les meilleurs paramètres, écrivons une boucle qui teste plusieurs combinaisons de `eps` et `min_samples` :

```python
import numpy as np

results = []

# Définir les plages de valeurs pour epsilon et min_samples
eps_values = np.arange(0.1, 2.1, 0.1)
min_samples_values = range(2, 11)

# Boucle sur toutes les combinaisons de eps et min_samples
for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan.fit(scaled_data)
        labels = dbscan.labels_
        
        # Calculer le nombre de clusters et de points de bruit
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        # Calculer le score de silhouette
        if n_clusters > 1:
            score = silhouette_score(scaled_data, labels)
        else:
            score = -1
        
        # Stocker les résultats
        results.append((eps, min_samples, n_clusters, n_noise, score))

# Convertir les résultats en DataFrame
results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters', 'n_noise', 'silhouette_score'])
```

### Étape 8 : Trouver les Meilleurs Résultats

Trier les résultats pour trouver la meilleure combinaison de `eps` et `min_samples` basée sur le score de silhouette :

```python
# Trier les résultats par score de silhouette
sorted_results = results_df.sort_values(by='silhouette_score', ascending=False)

# Afficher les meilleurs résultats
print(sorted_results.head())
```

### Étape 9 : Appliquer le Meilleur Modèle DBSCAN

Appliquons le modèle DBSCAN avec les meilleurs paramètres trouvés :

```python
# Appliquer le meilleur modèle DBSCAN
best_eps = sorted_results.iloc[0]['eps']
best_min_samples = sorted_results.iloc[0]['min_samples']

best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
best_dbscan.fit(scaled_data)
best_labels = best_dbscan.labels_

# Ajouter les nouvelles étiquettes de clusters aux données originales
data['Best_Cluster'] = best_labels
```

### Étape 10 : Visualiser les Clusters

Enfin, visualisons les clusters en utilisant une carte de clusters :

```python
# Visualiser les clusters avec une carte de clusters
sns.clustermap(data[['Calories', 'Protein', 'Sodium', 'Fiber', 'Best_Cluster']].sort_values(by='Best_Cluster'),
               cmap='viridis', figsize=(10, 7), row_cluster=False, col_cluster=False)
plt.show()

# Afficher le nombre de points dans chaque cluster
print(data['Best_Cluster'].value_counts())
```

### Code Complet

Voici le code complet pour cette démonstration :

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Charger le jeu de données des céréales
data = pd.read_csv('cereal.csv')

# Sélectionner les caractéristiques numériques pertinentes
numeric_data = data[['Calories', 'Protein', 'Sodium', 'Fiber']]

# Normaliser les données
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Appliquer DBSCAN avec les paramètres par défaut
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan.fit(scaled_data)
labels = dbscan.labels_

# Ajouter les étiquettes de clusters aux données originales
data['Cluster'] = labels

# Calculer le score de silhouette
score = silhouette_score(scaled_data, labels)
print(f"Score de Silhouette : {score}")

# Ajuster les paramètres de DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=10)
dbscan.fit(scaled_data)
labels = dbscan.labels_

# Ajouter les nouvelles étiquettes de clusters aux données originales
data['Cluster'] = labels

# Recalculer le score de silhouette
score = silhouette_score(scaled_data, labels)
print(f"Score de Silhouette avec eps=0.3 et min_samples=10 : {score}")

# Boucle pour tester différents paramètres de DBSCAN
results = []
eps_values = np.arange(0.1, 2.1, 0.1)
min_samples_values = range(2, 11)

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        dbscan.fit(scaled_data)
        labels = dbscan.labels_
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        if n_clusters > 1:
            score = silhouette_score(scaled_data, labels)
        else:
            score = -1
        
        results.append((eps, min_samples, n_clusters, n_noise, score))

results_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'n_clusters', 'n_noise', 'silhouette_score'])

# Trier les résultats par score de silhouette
sorted_results = results_df.sort_values(by='silhouette_score', ascending=False)
print(sorted_results.head())

# Appliquer le meilleur modèle DBSCAN
best_eps = sorted_results.iloc[0]['eps']
best_min_samples = sorted_results.iloc[0]['min_samples']

best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
best_dbscan.fit(scaled_data)
best_labels = best_dbscan.labels_

# Ajouter les nouvelles étiquettes de clusters aux données originales
data['Best_Cluster'] = best_labels

# Visualiser les clusters avec une carte de clusters
sns.clustermap(data[['Calories', 'Protein', 'Sodium', 'Fiber', 'Best_Cluster']].sort_values(by='Best_Cluster'),
               cmap='viridis', figsize=(10, 7), row_cluster=False, col_cluster=False)
plt.show()

# Afficher le nombre de points dans chaque cluster
print(data['Best_Cluster'].value_counts())
```

En suivant ces étapes, vous pouvez utiliser DBSCAN et le score de silhouette pour trouver et optimiser le meilleur modèle DBSCAN pour vos données.
