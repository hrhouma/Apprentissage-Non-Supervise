
# Démonstration : Clustering hiérarchique avec Scikit-Learn

### Introduction
Pour cette démonstration, nous allons réaliser un clustering hiérarchique en utilisant Scikit-Learn. Nous choisirons d'utiliser Scikit-Learn pour faciliter la comparaison avec d'autres modèles de machine learning et assurer une cohérence dans le format des données.

### Pourquoi Choisir Scikit-Learn vs SciPy ?
- **SciPy** : Utile pour créer des visualisations de dendrogrammes.
- **Scikit-Learn** : Idéal pour la création et la comparaison de modèles de machine learning grâce à un format de code cohérent.

### Étape 1 : Importation des Bibliothèques
Commencez par importer les bibliothèques nécessaires. Nous allons utiliser `AgglomerativeClustering` de Scikit-Learn et quelques outils de visualisation.

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from collections import Counter

# Générer des données synthétiques
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=0)

# Afficher les données
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title("Données Synthétiques")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### Étape 2 : Application du Clustering Hiérarchique Agglomératif
Nous allons maintenant appliquer le clustering hiérarchique agglomératif en utilisant Scikit-Learn.

```python
# Importer la classe AgglomerativeClustering
from sklearn.cluster import AgglomerativeClustering

# Instancier le modèle de clustering agglomératif
model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')

# Ajuster le modèle aux données
model.fit(X)

# Afficher les étiquettes de cluster
labels = model.labels_
print(labels)

# Visualiser les clusters
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow', s=50)
plt.title("Clustering Hiérarchique avec Scikit-Learn")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```

### Analyse des Résultats
Nous pouvons utiliser la bibliothèque `collections` pour compter le nombre de points dans chaque cluster.

```python
# Compter le nombre de points dans chaque cluster
counts = Counter(labels)
print(counts)
```

### Utilisation d'un Autre Jeu de Données
Maintenant, nous allons appliquer le clustering hiérarchique sur un autre jeu de données.

```python
# Générer un nouveau jeu de données synthétiques
X2, y2 = make_blobs(n_samples=150, centers=4, cluster_std=0.60, random_state=42)

# Afficher les données
plt.scatter(X2[:, 0], X2[:, 1], s=50)
plt.title("Nouveau Jeu de Données Synthétiques")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Calculer les liens hiérarchiques en utilisant la méthode de Ward avec SciPy
from scipy.cluster.hierarchy import dendrogram, linkage
Z = linkage(X2, method='ward')

# Créer le dendrogramme
plt.figure(figsize=(10, 7))
dendrogram(Z)
plt.title("Dendrogramme du Nouveau Jeu de Données")
plt.xlabel("Points de Données")
plt.ylabel("Distance Euclidienne")
plt.show()
```

### Détermination du Nombre de Clusters
En regardant le dendrogramme, nous pouvons déterminer le nombre optimal de clusters.

```python
# Ajuster le seuil de couleur pour visualiser les clusters
plt.figure(figsize=(10, 7))
dendrogram(Z, color_threshold=10)
plt.title("Dendrogramme avec Seuil de Couleur")
plt.xlabel("Points de Données")
plt.ylabel("Distance Euclidienne")
plt.show()
```

### Application du Modèle avec Scikit-Learn
En utilisant l'information obtenue du dendrogramme, nous allons spécifier le nombre de clusters et ajuster le modèle.

```python
# Instancier le modèle de clustering agglomératif avec 4 clusters
model2 = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')

# Ajuster le modèle aux nouvelles données
model2.fit(X2)

# Afficher les étiquettes de cluster
labels2 = model2.labels_
print(labels2)

# Visualiser les clusters
plt.scatter(X2[:, 0], X2[:, 1], c=labels2, cmap='rainbow', s=50)
plt.title("Clustering Hiérarchique avec Scikit-Learn sur le Nouveau Jeu de Données")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Compter le nombre de points dans chaque cluster
counts2 = Counter(labels2)
print(counts2)
```

### Conclusion
Nous avons démontré comment réaliser un clustering hiérarchique en utilisant Scikit-Learn et comment comparer les résultats avec ceux obtenus via SciPy. En utilisant SciPy, nous avons pu visualiser un dendrogramme pour déterminer le nombre optimal de clusters, puis appliquer ce nombre dans un modèle de clustering agglomératif avec Scikit-Learn. Cette approche permet de tirer parti des avantages des deux bibliothèques pour une analyse complète et cohérente des données.
