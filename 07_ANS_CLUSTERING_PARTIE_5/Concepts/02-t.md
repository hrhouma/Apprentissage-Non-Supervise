   $$
   \text{similarité}_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma_i^2}\right)
   $$

   $$
   P_{j|i} = \frac{\exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma_i^2}\right)}{\sum_{k \neq i} \exp\left(-\frac{\|x_i - x_k\|^2}{2\sigma_i^2}\right)}
   $$

   $$
   P_{ij} = \frac{P_{j|i} + P_{i|j}}{2N}
   $$


   $$
   Q_{ij} = \frac{\left(1 + \|y_i - y_j\|^2\right)^{-1}}{\sum_{k \neq l} \left(1 + \|y_k - y_l\|^2\right)^{-1}}
   $$

   $$
   KL(P||Q) = \sum_{i \neq j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
   $$

### Vue d'ensemble
Le t-SNE (t-distributed Stochastic Neighbor Embedding) est une technique de réduction de dimension non linéaire principalement utilisée pour la visualisation des données à haute dimension. Il transforme des données complexes en une représentation de faible dimension tout en préservant la structure des relations entre les points de données.

### Équations

1. **Similitude entre les points de données dans l'espace de haute dimension :**
2. **Probabilités conditionnelles :**
3. **Probabilités symétriques :**
4. **Probabilités dans l'espace de faible dimension :**
5. **Divergence de Kullback-Leibler (KL) :**


### Fonctionnement de t-SNE

1. **Similitudes Par Paires** :
   - **Calcul des Similitudes** : Pour chaque paire de points de données dans l'espace de haute dimension, t-SNE calcule une mesure de similarité en utilisant l'équation (1).

2. **Distribution de Probabilité Conjointe** :
   - **Probabilités Conditionnelles** : Les similarités sont ensuite converties en probabilités conditionnelles $$P_{j|i}$$ en utilisant l'équation (2).
   - **Probabilités Symétriques** : Ces probabilités conditionnelles sont symétrisées pour obtenir une distribution de probabilité conjointe $$P_{ij}$$, comme décrit par l'équation (3).

3. **Mappage en Faible Dimension** :
   - **Initialisation** : Les points de données sont initialement placés aléatoirement dans un espace de faible dimension (2D ou 3D).
   - **Probabilités en 2D/3D** : On calcule également des probabilités conjointes $$Q_{ij}$$ dans cet espace de faible dimension en utilisant l'équation (4).

4. **KL-Divergence** :
   - **Minimisation de la Divergence de KL** : L'objectif est de minimiser la divergence de Kullback-Leibler (KL) entre les distributions $$P$$ et $$Q$$, comme défini par l'équation (5).
   - **Descente de Gradient** : On utilise des techniques de descente de gradient pour ajuster les positions des points dans l'espace de faible dimension afin de minimiser cette divergence. Cela implique de déplacer les points de manière itérative pour réduire l'écart entre $$P$$ et $$Q$$.

### Applications

1. **Exploration des Données** :
   - **Visualisation** : Le t-SNE est utilisé pour visualiser des structures et des motifs dans des données complexes, révélant des clusters naturels qui ne sont pas visibles dans l'espace de haute dimension.

2. **Détection d'Anomalies** :
   - **Identification** : Les points de données qui sont éloignés des clusters principaux peuvent être identifiés comme des anomalies ou des outliers, facilitant la détection de cas exceptionnels.

3. **Prétraitement** :
   - **Réduction de Dimension** : Avant d'appliquer d'autres algorithmes de machine learning, le t-SNE peut réduire la dimension des données tout en conservant les relations importantes, améliorant ainsi les performances des modèles en réduisant la complexité.

### Exemple Concret

Prenons un ensemble de fruits avec des caractéristiques (couleur et taille) :
- Pomme rouge petite (x1 = [1, 1])
- Banane jaune grande (x2 = [5, 5])
- Cerise rouge petite (x3 = [1, 1])

1. **Calcul des Distances** :
   - Distance entre Pomme et Cerise :
$$
\|x_1 - x_3\| = \sqrt{(1-1)^2 + (1-1)^2} = 0
$$
   - Distance entre Pomme et Banane :
$$ 
\|x_1 - x_2\| = \sqrt{(1-5)^2 + (1-5)^2} = \sqrt{32} \approx 5.66
$$

2. **Création de Probabilités** :
   - Pomme et Cerise : Probabilité élevée (proche de 1)
   - Pomme et Banane : Probabilité faible (proche de 0)

3. **Initialisation en 2D** :
   - Placer les fruits aléatoirement sur une feuille (espace 2D).

4. **Réajustement** :
   - Utiliser la descente de gradient pour ajuster les positions et minimiser la divergence de KL, assurant que les distances en 2D reflètent les distances calculées.

En conclusion, le t-SNE transforme des données complexes en représentations visuelles simplifiées, tout en préservant les relations entre les points de données, ce qui facilite l'analyse et la compréhension des structures sous-jacentes.
