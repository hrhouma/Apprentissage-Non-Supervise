### Vue d'ensemble
Le t-SNE (t-distributed Stochastic Neighbor Embedding) est une technique de réduction de dimension non linéaire principalement utilisée pour la visualisation des données à haute dimension. Il transforme des données complexes en une représentation de faible dimension tout en préservant la structure des relations entre les points de données.

### Fonctionnement de t-SNE

1. **Similitudes Par Paires** :
   - **Calcul des Similitudes** : Pour chaque paire de points de données dans l'espace de haute dimension, t-SNE calcule une mesure de similarité. Typiquement, on utilise la distance Euclidienne pour quantifier à quel point les points sont proches.
     $$
     \text{similarité}_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma_i^2}\right)
     $$
     où $$x_i$$ et $$x_j$$ sont les vecteurs de caractéristiques des points $$i$$ et $$j$$, et $$\sigma_i$$ est un paramètre d'échelle ajusté pour chaque point $$i$$.

2. **Distribution de Probabilité Conjointe** :
   - **Probabilités Conditionnelles** : Les similarités sont ensuite converties en probabilités conditionnelles $$P_{j|i}$$ qui représentent la probabilité que le point $$j$$ soit le voisin du point $$i$$.
     $$
     P_{j|i} = \frac{\exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma_i^2}\right)}{\sum_{k \neq i} \exp\left(-\frac{\|x_i - x_k\|^2}{2\sigma_i^2}\right)}
     $$
   - **Probabilités Symétriques** : Ces probabilités conditionnelles sont symétrisées pour obtenir une distribution de probabilité conjointe $$P_{ij}$$.
     $$
     P_{ij} = \frac{P_{j|i} + P_{i|j}}{2N}
     $$
     où $$N$$ est le nombre total de points de données.

3. **Mappage en Faible Dimension** :
   - **Initialisation** : Les points de données sont initialement placés aléatoirement dans un espace de faible dimension (2D ou 3D).
   - **Probabilités en 2D/3D** : On calcule également des probabilités conjointes $$Q_{ij}$$ dans cet espace de faible dimension, utilisant une distribution de Student-t avec un degré de liberté pour accentuer les différences.
     $$
     Q_{ij} = \frac{\left(1 + \|y_i - y_j\|^2\right)^{-1}}{\sum_{k \neq l} \left(1 + \|y_k - y_l\|^2\right)^{-1}}
     $$
     où $$y_i$$ et $$y_j$$ sont les positions des points dans l'espace de faible dimension.

4. **KL-Divergence** :
   - **Minimisation de la Divergence de KL** : L'objectif est de minimiser la divergence de Kullback-Leibler (KL) entre les distributions $$P$$ et $$Q$$.
     $$
     KL(P||Q) = \sum_{i \neq j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
     $$
   - **Descente de Gradient** : On utilise des techniques de descente de gradient pour ajuster les positions des points dans l'espace de faible dimension afin de minimiser cette divergence. Cela implique de déplacer les points de manière itérative pour réduire l'écart entre $$P$$ et $$Q$$.

### Applications

1. **Exploration des Données** :
   - **Visualisation** : Le t-SNE est utilisé pour visualiser des structures et des motifs dans des données complexes, révélant des clusters naturels qui ne sont pas visibles dans l'espace de haute dimension.

2. **Détection d'Anomalies** :
   - **Identification** : Les points de données qui sont éloignés des clusters principaux peuvent être identifiés comme des anomalies ou des outliers, facilitant la détection de cas exceptionnels.

3. **Prétraitement** :
   - **Réduction de Dimension** : Avant d'appliquer d'autres algorithmes de machine learning, le t-SNE peut réduire la dimension des données tout en conservant les relations importantes, améliorant ainsi les performances des modèles en réduisant la complexité.

### Exemple Concret

Prenons un ensemble de fruits avec des caractéristiques (couleur et taille) :
- Pomme rouge petite (x1 = [1, 1])
- Banane jaune grande (x2 = [5, 5])
- Cerise rouge petite (x3 = [1, 1])

1. **Calcul des Distances** :
   - Distance entre Pomme et Cerise : $$ \|x_1 - x_3\| = \sqrt{(1-1)^2 + (1-1)^2} = 0 $$
   - Distance entre Pomme et Banane : $$ \|x_1 - x_2\| = \sqrt{(1-5)^2 + (1-5)^2} = \sqrt{32} \approx 5.66 $$

2. **Création de Probabilités** :
   - Pomme et Cerise : Probabilité élevée (proche de 1)
   - Pomme et Banane : Probabilité faible (proche de 0)

3. **Initialisation en 2D** :
   - Placer les fruits aléatoirement sur une feuille (espace 2D).

4. **Réajustement** :
   - Utiliser la descente de gradient pour ajuster les positions et minimiser la divergence de KL, assurant que les distances en 2D reflètent les distances calculées.

En conclusion, le t-SNE transforme des données complexes en représentations visuelles simplifiées, tout en préservant les relations entre les points de données, ce qui facilite l'analyse et la compréhension des structures sous-jacentes.
