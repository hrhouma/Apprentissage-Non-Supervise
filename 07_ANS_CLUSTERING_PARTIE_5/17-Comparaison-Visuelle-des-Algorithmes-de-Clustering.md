# Comparaison Visuelle des Algorithmes de Clustering

## Installation des Bibliothèques Nécessaires

Tout d'abord, assurez-vous que les bibliothèques nécessaires sont installées. Vous pouvez les installer en exécutant la commande suivante :

```python
!pip install numpy matplotlib scikit-learn
```

## Importation des Bibliothèques

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.datasets import make_blobs, make_moons, make_circles
```

## Fonction pour Tracer les Clusters

Cette fonction nous permettra de visualiser les résultats des différents algorithmes de clustering.

```python
def plot_clusters(X, labels, title, ax):
    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    ax.set_title(title)
```

## Génération des Données

Nous allons générer quatre types de jeux de données pour illustrer les performances des algorithmes de clustering sur différentes configurations.

```python
spherical_data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
chain_data, _ = make_moons(n_samples=300, noise=0.05, random_state=0)
circle_data, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=0)
random_data = np.random.rand(300, 2)
```

## Configuration des Modèles de Clustering

Nous allons configurer les trois algorithmes de clustering que nous allons comparer.

```python
kmeans = KMeans(n_clusters=4)
hierarchical = AgglomerativeClustering(n_clusters=4)
dbscan = DBSCAN(eps=0.3, min_samples=5)
```

## Jeux de Données

Nous allons créer une liste de nos jeux de données pour faciliter leur traitement dans une boucle.

```python
datasets = [
    ('Clusters sphériques', spherical_data),
    ('Clusters en forme de longues chaînes', chain_data),
    ('Clusters en forme de cercle', circle_data),
    ('Données aléatoires', random_data)
]
```

## Visualisation des Clusters

Nous allons tracer les résultats des trois algorithmes de clustering pour chaque jeu de données.

```python
fig, axes = plt.subplots(len(datasets), 3, figsize=(15, 20))
for idx, (title, data) in enumerate(datasets):
    kmeans_labels = kmeans.fit_predict(data)
    hierarchical_labels = hierarchical.fit_predict(data)
    dbscan_labels = dbscan.fit_predict(data)

    plot_clusters(data, kmeans_labels, f'{title}\nK-means : Excellent pour des clusters sphériques bien séparés.', axes[idx, 0])
    plot_clusters(data, hierarchical_labels, f'{title}\nClustering Hiérarchique : Identifie correctement les clusters.', axes[idx, 1])
    plot_clusters(data, dbscan_labels, f'{title}\nDBSCAN : Identifie correctement les clusters.', axes[idx, 2])

plt.tight_layout()
plt.show()
```

## Explications des Résultats

1. **Clusters sphériques :**
    - **K-means** : Excellent pour des clusters sphériques bien séparés.
    - **Clustering Hiérarchique** : Identifie correctement les clusters.
    - **DBSCAN** : Identifie correctement les clusters.

2. **Clusters en forme de longues chaînes :**
    - **K-means** : Essaye de trouver des clusters sphériques même s'ils n'existent pas.
    - **Clustering Hiérarchique** : Fait un meilleur travail en trouvant des clusters.
    - **DBSCAN** : Identifie correctement les clusters.

3. **Clusters en forme de cercle :**
    - **K-means** : Essaye de trouver des clusters sphériques, ce qui n'est pas adapté ici.
    - **Clustering Hiérarchique** : Identifie correctement les clusters.
    - **DBSCAN** : Le plus performant, identifie les clusters irréguliers et les points de bruit.

4. **Données aléatoires :**
    - **K-means** : Essaye de trouver des clusters là où il n'y en a pas.
    - **Clustering Hiérarchique** : Identifie un cluster unique dans une zone légèrement différente.
    - **DBSCAN** : Reconnaît que tout est du bruit.

## Conclusion

En utilisant ces différentes méthodes de clustering, nous avons pu voir comment chaque algorithme se comporte avec différents jeux de données. En comparant les scores de silhouette, nous pouvons déterminer quel algorithme a produit les clusters les plus distincts et les plus appropriés pour un jeu de données spécifique.

---

# ANNEXE : 

```python
# -*- coding: utf-8 -*-
"""Comparaison visuelle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZbNdx-JsZEFBspMZKstZtAj_QR6l3iSV
"""

!pip install numpy matplotlib scikit-learn

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.datasets import make_blobs, make_moons, make_circles

# Function to plot the clusters
def plot_clusters(X, labels, title, ax):
    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    ax.set_title(title)

# Generate data
spherical_data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
chain_data, _ = make_moons(n_samples=300, noise=0.05, random_state=0)
circle_data, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=0)
random_data = np.random.rand(300, 2)

# Set up clustering models
kmeans = KMeans(n_clusters=4)
hierarchical = AgglomerativeClustering(n_clusters=4)
dbscan = DBSCAN(eps=0.3, min_samples=5)

datasets = [
    ('Spherical Clusters', spherical_data),
    ('Chain-shaped Clusters', chain_data),
    ('Circular Clusters', circle_data),
    ('Random Data', random_data)
]

fig, axes = plt.subplots(len(datasets), 3, figsize=(15, 20))
for idx, (title, data) in enumerate(datasets):
    kmeans_labels = kmeans.fit_predict(data)
    hierarchical_labels = hierarchical.fit_predict(data)
    dbscan_labels = dbscan.fit_predict(data)

    plot_clusters(data, kmeans_labels, f'{title}\nK-means', axes[idx, 0])
    plot_clusters(data, hierarchical_labels, 'Hierarchical Clustering', axes[idx, 1])
    plot_clusters(data, dbscan_labels, 'DBSCAN', axes[idx, 2])

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.datasets import make_blobs, make_moons, make_circles

# Comparaison visuelle
# Pour montrer visuellement comment ces modèles se comparent côte à côte, nous allons utiliser des visualisations pour comparer nos modèles sur différents jeux de données.

# Clusters sphériques
# Clustering sphériques
# K-means : Excellent pour des clusters sphériques bien séparés.
# Clustering Hiérarchique : Identifie correctement les clusters.
# DBSCAN : Identifie correctement les clusters.

# Clusters en forme de longues chaînes
# Clustering longues chaînes
# K-means : Essaye de trouver des clusters sphériques même s'ils n'existent pas.
# Clustering Hiérarchique : Fait un meilleur travail en trouvant des clusters.
# DBSCAN : Identifie correctement les clusters.

# Clusters en forme de cercle
# Clustering cercles
# K-means : Essaye de trouver des clusters sphériques, ce qui n'est pas adapté ici.
# Clustering Hiérarchique : Identifie correctement les clusters.
# DBSCAN : Le plus performant, identifie les clusters irréguliers et les points de bruit.

# Clusters de formes aléatoires
# Clustering formes aléatoires
# K-means : Essaye de trouver des clusters sphériques, ce qui n'est pas adapté ici.
# Clustering Hiérarchique : Identifie correctement les clusters.
# DBSCAN : Le plus performant, identifie les clusters irréguliers et les points de bruit.

# Données aléatoires
# Clustering données aléatoires
# K-means : Essaye de trouver des clusters là où il n'y en a pas.
# Clustering Hiérarchique : Identifie un cluster unique dans une zone légèrement différente.
# DBSCAN : Reconnaît que tout est du bruit.

# Conclusion
# En utilisant ces différentes méthodes de clustering, nous avons pu voir comment chaque algorithme se comporte avec différents jeux de données. En comparant les scores de silhouette, nous pouvons déterminer quel algorithme a produit les clusters les plus distincts et les plus appropriés pour un jeu de données spécifique.

# Fonction pour tracer les clusters
def plot_clusters(X, labels, title, ax):
    ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50)
    ax.set_title(title)

# Générer les données
spherical_data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
chain_data, _ = make_moons(n_samples=300, noise=0.05, random_state=0)
circle_data, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=0)
random_data = np.random.rand(300, 2)

# Configurer les modèles de clustering
kmeans = KMeans(n_clusters=4)
hierarchical = AgglomerativeClustering(n_clusters=4)
dbscan = DBSCAN(eps=0.3, min_samples=5)

datasets = [
    ('Clusters sphériques', spherical_data),
    ('Clusters en forme de longues chaînes', chain_data),
    ('Clusters en forme de cercle', circle_data),
    ('Données aléatoires', random_data)
]

fig, axes = plt.subplots(len(datasets), 3, figsize=(15, 20))
for idx, (title, data) in enumerate(datasets):
    kmeans_labels = kmeans.fit_predict(data)
    hierarchical_labels = hierarchical.fit_predict(data)
    dbscan_labels = dbscan.fit_predict(data)

    plot_clusters(data, kmeans_labels, f'{title}\nK-means : Excellent pour des clusters sphériques bien séparés.', axes[idx, 0])
    plot_clusters(data, hierarchical_labels, f'{title}\nClustering Hiérarchique : Identifie correctement les clusters.', axes[idx, 1])
    plot_clusters(data, dbscan_labels, f'{title}\nDBSCAN : Identifie correctement les clusters.', axes[idx, 2])

plt.tight_layout()
plt.show()
```
