# Table des Matières

1. [Formule 1: Z-Score](#formule-1)
2. [Formule 2: Intervalle Interquartile (IQR)](#formule-2)
3. [Formule 3: Distance Euclidienne](#formule-3)
4. [Formule 4: Distance de Manhattan](#formule-4)
5. [Formule 5: Distance de Mahalanobis](#formule-5)
6. [Section 1: Introduction à la détection d'anomalies](#section-1)
7. [Section 2: Comprendre les anomalies](#section-2)
8. [Section 3: Exercice Pratique](#section-3)
9. [Section 4: Détection des Outliers](#section-4)
10. [Section 5: Algorithmes Non Basés sur le Clustering](#section-5)
11. [Section 6: Explication des Résultats de la Détection d'Anomalies dans un Contexte de Fraude](#section-6)
12. [Section 7: Comparaison des Algorithmes de Détection d'Anomalies avec PyOD](#section-7)

---

# FORMULES 

---

## Formule 1: Z-Score
<a name="formule-1"></a>

$$
Z = \frac{(X - \mu)}{\sigma}
$$
  
  où \(Z\) est le Z-score, \(X\) est la valeur du point de données, \(\mu\) est la moyenne de l'ensemble de données, et \(\sigma\) est l'écart-type.

[Retour en haut](#table-des-matières)

---

## Formule 2: Intervalle Interquartile (IQR)
<a name="formule-2"></a>

$$
IQR = Q3 - Q1
$$

 où \(Q3\) est le troisième quartile (75e percentile) et \(Q1\) est le premier quartile (25e percentile).

[Retour en haut](#table-des-matières)
 
---

## Formule 3: Distance Euclidienne
<a name="formule-3"></a>

$$
d = \sqrt{\sum_{i=1}^{n}(X_i - Y_i)^2}
$$

  où \(d\) est la distance Euclidienne entre les points \(X\) et \(Y\).

[Retour en haut](#table-des-matières)

---

## Formule 4: Distance de Manhattan
<a name="formule-4"></a>

$$
d = \sum_{i=1}^{n} |X_i - Y_i|
$$

où \(d\) est la distance de Manhattan entre les points \(X\) et \(Y\).

[Retour en haut](#table-des-matières)

---

## Formule 5: Distance de Mahalanobis
<a name="formule-5"></a>

$$
d_M = \sqrt{(X - \mu)^T \Sigma^{-1} (X - \mu)}
$$

$$ 
\mu
$$ 

est le vecteur de la moyenne.

$$
\Sigma
$$

est la matrice de covariance.

[Retour en haut](#table-des-matières)

---

# Section 1: Introduction à la détection d'anomalies
<a name="section-1"></a>

- La détection d'anomalies est un domaine de plus en plus adopté dans l'industrie.
- Au cœur de ce concept, une anomalie est quelque chose qui ne correspond pas au reste des données ou des transactions.
- Ce principe est largement applicable dans différents secteurs, tels que l'industrie manufacturière sous forme de maintenance prédictive, ainsi que dans la détection de fraudes.
- Cette présentation commence par une compréhension des anomalies et des différents types d'anomalies.
- La présentation est structurée pour introduire les trois types de détection d'anomalies et les domaines d'application associés.
- Nous aborderons ensuite le codage pertinent pour les types de détection en apprentissage non supervisé.
- Les données et les codes utilisés dans cette présentation sont disponibles dans le dossier ressources, vous permettant ainsi de les télécharger et de pratiquer en même temps, dans la deuxième partie de la séance.

[Retour en haut](#table-des-matières)

---

# Section 2: Comprendre les anomalies
<a name="section-2"></a>

- Avant de plonger dans les algorithmes, il est essentiel de comprendre ce que sont les anomalies.
- Une anomalie peut être un ou plusieurs points de données qui ne s'intègrent pas au reste des données.
- Identifier des anomalies dans un petit ensemble de données est généralement facile, mais cela devient difficile avec des ensembles de données **plus volumineux**.
- Il existe trois types principaux d'anomalies :
  - **Anomalies basées sur le temps** : Les points de données sont dépendants du temps. Par exemple, le prix de l'essence à différents jours d'un mois.
  - **Anomalies non basées sur le temps** : Les points de données ne sont pas dépendants du temps. Par exemple, le prix d'un appartement dans une ville donnée en fonction de plusieurs facteurs.
  - **Anomalies d'image** : Détecter des anomalies dans un ensemble d'images, comme pour les données classiques.
- Les anomalies peuvent être détectées à l'aide d'algorithmes supervisés ou non supervisés :
  - **Supervisé** : Lorsque des références passées d'anomalies sont disponibles.
  - **Non supervisé** : Lorsque ces références ne sont pas disponibles, ce qui est souvent le cas en entreprise.
- Les algorithmes non supervisés sont souvent préférés en raison de l'absence de données robustes. Ils peuvent être basés sur des clusters ou non.
- Exemples de jeux de données :
  - **Jeux de données basés sur le temps** : Par exemple, les transactions d'un client bancaire à différents moments.
  - **Jeux de données non basés sur le temps** : Par exemple, le prix d'un appartement en fonction de différents facteurs sans référence au temps.
- La détection d'anomalies basée sur le temps est particulièrement utile pour identifier des événements rares mais critiques, comme la maintenance prédictive dans l'industrie.
- **Concept d'outliers (valeurs aberrantes)** :
  - Les outliers sont des points de données extrêmes.
  - Chaque outlier est une anomalie, mais toutes les anomalies ne sont pas des outliers (voir la discussion dans l'**annexe 1**).
  - Techniques de détection des outliers :
    - **Box plot** : Utilisation de l'intervalle interquartile.
    - **Diagramme de contrôle** : Utilisation de la moyenne et de l'écart-type pour définir des limites de contrôle.
- En résumé, bien que chaque outlier soit une anomalie, l'inverse n'est pas toujours vrai.

[Retour en haut](#table-des-matières)

---

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Section 3: Exercice Pratique
⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️

<a name="section-3"></a>




**Instructions de l'exercice**

- Temps alloué : 30 minutes

- Dans cet exercice, vous êtes invités à fournir des exemples d'application des anomalies et de la détection d'anomalies dans la vie réelle (Après, vous pouvez consulter le document 02-exemples-d-applications-.. dans le même dossier ici). 
- Cet exercice vous aidera dans votre parcours d'apprentissage, car vous pouvez de vous référer à ces exemples tout au long de ce cours et aussi évaluer l'importance de la détection d'anomalies dans des situations réelles.

**Questions pour cet exercice :**

1. Décrivez ce qu'est une anomalie avec vos propres mots.
2. Donnez des exemples d'anomalies basées sur le temps. Les exemples peuvent provenir de votre domaine professionnel ou d'un domaine que vous connaissez bien.
3. Donnez des exemples d'anomalies non basées sur le temps (non supervisées). Les exemples peuvent provenir de votre domaine professionnel ou d'un domaine que vous connaissez bien.
4. Donnez des exemples d'anomalies non basées sur le temps (supervisées). Les exemples peuvent provenir de votre domaine professionnel ou d'un domaine que vous connaissez bien.
5. Donnez des exemples d'anomalies basées sur les images. Les exemples peuvent provenir de votre domaine professionnel ou d'un domaine que vous connaissez bien.

[Retour en haut](#table-des-matières)

---

**Proposition de l'instructeur :**

1. **Décrivez ce qu'est une anomalie avec vos propres mots.**
   - Une anomalie est un point de données qui ne correspond pas ou ne s'intègre pas bien avec les autres points de données.

2. **Donnez des exemples d'anomalies basées sur le temps.**
   - <Il s'agit d'une question basée sur l'application des concepts. Il n'y a donc pas de réponse standard. Les apprenants doivent répondre en fonction de leur compréhension.>

3. **Donnez des exemples d'anomalies non basées sur le temps (non supervisées).**
   - <Il s'agit d'une question basée sur l'application des concepts. Il n'y a donc pas de réponse standard. Les apprenants doivent répondre en fonction de leur compréhension.>

4. **Donnez des exemples d'anomalies non basées sur le temps (supervisées).**
   - <Il s'agit d'une question basée sur l'application des concepts. Il n'y a donc pas de réponse standard. Les apprenants doivent répondre en fonction de

 leur compréhension.>

5. **Donnez des exemples d'anomalies basées sur les images.**
   - <Il s'agit d'une question basée sur l'application des concepts. Il n'y a donc pas de réponse standard. Les apprenants doivent répondre en fonction de leur compréhension.>

[Retour en haut](#table-des-matières)

---

# Section 4:

----

# Section 4.1 : Détection des Outliers
<a name="section-4"></a>

- Dans cette section, nous allons explorer différentes méthodes pour détecter les outliers, c'est-à-dire des points de données extrêmes qui se situent nettement en dehors de la majorité d'un ensemble de données ou d'un cluster. La détection des outliers est cruciale pour une analyse statistique précise et la performance des modèles.

## Méthode du Z-Score

- La méthode du Z-score est une technique statistique utilisée pour identifier les outliers en mesurant le nombre d'écarts-types d'un point de données par rapport à la moyenne de l'ensemble de données.

# Voir FORMULE 1

- **Interprétation :** Les points de données avec un Z-score supérieur à 3 ou inférieur à -3 sont considérés comme des outliers car ils se situent bien en dehors de la plage normale des données. Parfois, une valeur seuil de 2 au lieu de 3 est également utilisée.

- **Exemple :** Prenons un ensemble de données comprenant des identifiants de factures et leurs montants respectifs. En calculant le Z-score pour chaque montant, les valeurs avec un Z-score supérieur à 3 ou inférieur à -3 sont marquées comme outliers.

- **Programmation :** En Python, cela peut être accompli en utilisant les bibliothèques `numpy` et `pandas`. Vous calculez d'abord la moyenne et l'écart-type, puis vous appliquez la formule du Z-score pour chaque point de données.

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 1 - 01-ZSCORE
⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️

- https://drive.google.com/drive/folders/1H7kbeWT_kd-vCTm3jFhZygDMAl28kdJn?usp=sharing

## Méthode de l'Intervalle Interquartile (IQR)

- La méthode de l'Intervalle Interquartile (IQR) identifie les outliers en mesurant l'étendue des 50 % de données centrales. L'IQR est calculé comme la différence entre le troisième quartile (\(Q3\)) et le premier quartile (\(Q1\)).

# Voir FORMULE 2

- **Interprétation :** Les valeurs situées en dessous de \(Q1 - 1.5 \times IQR\) ou au-dessus de \(Q3 + 1.5 \times IQR\) sont considérées comme des outliers.

- **Exemple :** Dans notre ensemble de données de factures, les montants situés en dehors de ces bornes (par exemple, en dessous de 77.5 ou au-dessus de 105.5) sont identifiés comme des outliers.

- **Programmation :** Vous pouvez calculer les quantiles avec la méthode `quantile()` de `pandas`, puis identifier les outliers en appliquant la formule de l'IQR.


## Annexe pour IQR pour celles et ceux qui ont des difficultés en mathématiques: 

- L'Intervalle Interquartile (IQR) est une mesure de dispersion statistique qui montre l'écart entre les valeurs du premier quartile (Q1) et du troisième quartile (Q3). Pour l'expliquer à quelqu'un qui ne comprend pas bien les statistiques, on peut utiliser une analogie simple.

- Imagine que tu as un groupe d'enfants dans une cour de récréation. Chaque enfant a un jouet, et tu veux savoir si la plupart des enfants ont des jouets similaires en taille. Si on classe tous les jouets du plus petit au plus grand, le quartile 1 (Q1) représente la taille en dessous de laquelle se trouve le premier quart des jouets les plus petits. Le quartile 3 (Q3) représente la taille en dessous de laquelle se trouve les trois quarts des jouets, c'est-à-dire que seuls les plus grands jouets sont au-dessus de cette taille.

- L'IQR, ou l'Intervalle Interquartile, est simplement la différence entre la taille du jouet au niveau de Q3 et celle au niveau de Q1. Cela te donne une idée de la taille moyenne des jouets qui ne sont ni trop petits ni trop grands, c'est-à-dire des jouets qui se situent au milieu de la distribution. Si cet intervalle est petit, ça veut dire que la plupart des jouets sont de tailles similaires. S'il est grand, ça signifie qu'il y a une grande variation de taille entre les jouets.

- Dans la vraie vie, l'IQR peut être utilisé pour évaluer la "consistance" des données. Par exemple, si tu regardes les notes des élèves dans une classe, l'IQR te dirait à quel point les notes de la majorité des élèves sont proches les unes des autres, en éliminant les notes les plus extrêmes (les plus basses et les plus hautes).


- Simplifions encore :

- Imagine que tu as 100 biscuits de tailles différentes, que tu veux comparer. Tu les alignes du plus petit au plus grand. Le premier quart (les 25 premiers biscuits) représente les biscuits petits, et le troisième quart (les 25 derniers avant les plus grands) représente les biscuits moyens-grands.

- L'Intervalle Interquartile (IQR) te dit simplement de prendre la taille du 75ème biscuit (Q3) et de soustraire la taille du 25ème biscuit (Q1). Cela te donne une idée de la taille des biscuits qui se trouvent au milieu de ton rang, sans tenir compte des très petits ou des très grands.

- En gros, ça te montre à quel point les biscuits du milieu sont de tailles similaires. Si cet intervalle est petit, ça veut dire qu'ils sont presque tous de la même taille. S'il est grand, ça veut dire qu'ils varient beaucoup en taille.

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 2 - 02-IQR
⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️

## Méthode Basée sur la Distance

La méthode basée sur la distance identifie les outliers en mesurant la distance entre les points de données et la moyenne de l'ensemble de données. Il existe plusieurs types de distances couramment utilisées :

- **Distance Euclidienne :**  

# Voir FORMULE 3

- **Distance de Manhattan :**  

# Voir FORMULE 4

- **Distance de Mahalanobis :**  

# Voir FORMULE 5

- **Exemple :** Dans l'ensemble de données de factures, vous pouvez utiliser ces différentes distances pour identifier les points de données qui sont significativement éloignés de la moyenne, et donc les marquer comme outliers.

- **Programmation :** Utilisez `scipy.spatial.distance` pour calculer ces distances en Python.

## Méthode DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de clustering qui regroupe les points de données proches tout en identifiant les points dans les régions à faible densité comme des outliers.

- **Paramètres :** L'algorithme nécessite deux paramètres :
  - \(\epsilon\) : la distance maximale pour qu'un point soit considéré comme voisin d'un autre.
  - \(minPts\) : le nombre minimal de points requis pour former une région dense.

- **Types de points :**
  - **Point de cœur :** Un point avec au moins \(minPts\) voisins dans son voisinage défini par \(\epsilon\).
  - **Point de bordure :** Un point dans le voisinage d'un point de cœur mais avec moins de \(minPts\) voisins.
  - **Point de bruit :** Un point qui n'est ni un point de cœur ni un point de bordure, et qui est donc considéré comme un outlier.

- **Exemple :** En appliquant DBSCAN à un ensemble de données de factures, les montants isolés peuvent être identifiés comme des outliers en fonction de leur densité locale.

- **Programmation :** Utilisez `DBSCAN` de `scikit-learn` pour appliquer cet algorithme à vos données.

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 3- 03-DBSCAN-OUTLIER
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇


----

# Section 4.2 - Exercice 

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 4- RÉSUMÉ ZSCORE-IQR
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇



### Instructions de l'exercice
- D'abord. il faut télécharger le fichier excel dans le dossier ressources TAT.xlsx: 
1. **Z-Score Method :** Déterminez le nombre d'outliers en utilisant cette méthode.
2. **IQR Method :** Utilisez la méthode de l'IQR pour identifier les outliers.
3. **Distance Method :** Appliquez les distances Euclidienne, Manhattan et Mahalanobis pour détecter les outliers.
4. **DBSCAN :** Ajustez les paramètres \(\epsilon\) et \(minPts\) pour identifier les outliers et observez les résultats.



- Le "Turn Around Time" (TAT) est un terme couramment utilisé dans divers contextes, notamment en gestion de projet, en opérations et en informatique. Il se réfère généralement au temps total nécessaire pour accomplir une tâche ou un processus du début à la fin. 
- Exemples:
  
1. **Gestion de projet/Opérations** : Le "Turn Around Time" désigne le temps total écoulé entre l'initiation d'une tâche ou d'un processus et son achèvement. Par exemple, le temps écoulé entre le moment où un client passe une commande et celui où la commande est exécutée et livrée.

2. **Fabrication** : Le "Turn Around Time" peut faire référence au temps nécessaire pour passer d'une production à une autre sur une machine ou une ligne de production. Cela peut aussi indiquer le temps requis pour réparer ou entretenir un équipement.

3. **Soins de santé** : Dans le domaine de la santé, le "Turn Around Time" peut désigner le temps pris pour effectuer des tests médicaux et fournir les résultats au patient ou au professionnel de santé.

4. **Informatique/Ingénierie logicielle** : En informatique, le "Turn Around Time" peut désigner le temps total pris par un système informatique pour exécuter une tâche ou un processus depuis le moment où il est soumis jusqu'à ce que les résultats soient disponibles.

[Retour en haut](#table-des-matières)

----

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
### EXERCICE (04-(Section 4.2 - Exercice ) Exercice 1 - outlier detection - section4-2-TAT.xlsx)
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇

# Section 4.3 - proposition de correction pour la détection des Outliers

```python
# Importer les bibliothèques nécessaires
import pandas as pd
import numpy as np
from scipy.spatial import distance
from scipy.stats import zscore
from sklearn.covariance import EmpiricalCovariance
from sklearn.cluster import DBSCAN

# Charger les données à partir d'un fichier CSV
data = pd.read_csv('chemin_du_fichier.csv')

# Extraire la colonne 'Turn Around Time' (temps de rotation) pour l'analyse
tat = data['Turn Around Time']

# Méthode 1 : Z-Score
# Le Z-Score est utilisé pour normaliser les données et identifier les valeurs qui s'écartent 
# de la moyenne par plus de 3 écarts-types
data['z_score'] = zscore(tat)
z_outliers = data[np.abs(data['z_score']) > 3]  # Les outliers sont ceux avec un Z-Score absolu > 3

# Méthode 2 : IQR (Interquartile Range)
# Le IQR est la différence entre le troisième quartile (Q3) et le premier quartile (Q1)
# Cette méthode identifie les outliers en dehors de l'intervalle [Q1 - 1.5*IQR, Q3 + 1.5*IQR]
Q1 = tat.quantile(0.25)
Q3 = tat.quantile(0.75)
IQR = Q3 - Q1
iqr_outliers = data[(tat < (Q1 - 1.5 * IQR)) | (tat > (Q3 + 1.5 * IQR))]

# Méthode 3 : Méthode des distances
# Utilisation de trois types de distances pour identifier les outliers

# Calculer le vecteur moyen pour la distance
mean_vector = np.mean(tat)

# Calculer la matrice de covariance pour la distance de Mahalanobis
cov_matrix = np.cov(tat)
inv_cov_matrix = np.linalg.inv(cov_matrix.reshape(1, 1))

# 3.1) Distance de Manhattan
# Calcul de la distance de Manhattan pour chaque point et identification des outliers
manhattan_outliers = data[distance.cdist(tat.values.reshape(-1, 1), np.array([[mean_vector]]), 'cityblock').flatten() > 3]

# 3.2) Distance Euclidienne
# Calcul de la distance Euclidienne pour chaque point et identification des outliers
euclidean_outliers = data[distance.cdist(tat.values.reshape(-1, 1), np.array([[mean_vector]]), 'euclidean').flatten() > 3]

# 3.3) Distance de Mahalanobis
# Calcul de la distance de Mahalanobis pour chaque point et identification des outliers
mahalanobis_outliers = data[distance.cdist(tat.values.reshape(-1, 1), np.array([[mean_vector]]), 'mahalanobis', VI=inv_cov_matrix).flatten() > 3]

# Méthode 4 : DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
# DBSCAN est un algorithme de clustering qui peut identifier les points aberrants comme ceux qui ne font pas partie d'un cluster dense
dbscan = DBSCAN(eps=0.5, min_samples=5).fit(tat.values.reshape(-1, 1))
data['dbscan_labels'] = dbscan.labels_
dbscan_outliers = data[data['dbscan_labels'] == -1]  # Les outliers sont étiquetés avec -1 par DBSCAN

# Compter et afficher le nombre d'outliers détectés par chaque méthode
outlier_counts = {
    "Z-Score": len(z_outliers),
    "IQR": len(iqr_outliers),
    "Manhattan Distance": len(manhattan_outliers),
    "Euclidean Distance": len(euclidean_outliers),
    "Mahalanobis Distance": len(mahalanobis_outliers),
    "DBSCAN": len(dbscan_outliers)
}

print("Nombre d'outliers détectés par chaque méthode :")
for method, count in outlier_counts.items():
    print(f"{method}: {count}")
```

### Explications :

1. **Détermination du nombre d'outliers en utilisant différentes méthodes** :
   - **Méthode Z-Score** : Normalisation des données pour identifier les points qui s'écartent significativement de la moyenne.
   - **Méthode IQR (Interquartile Range)** : Utilisation des quartiles pour définir un intervalle de valeurs "normales" et identifier les valeurs extrêmes.
   - **Méthode des distances** : Application de trois distances différentes (Manhattan, Euclidienne, Mahalanobis) pour identifier les outliers basés sur la distance par rapport à un centre.
   - **DBSCAN** : Algorithme de clustering qui identifie les outliers en tant que points isolés ne faisant partie d'aucun cluster dense.

2. **Modification des paramètres de DBSCAN pour ajuster la détection des outliers** :
   - Test de différentes valeurs pour epsilon (la distance maximale entre deux points pour être considérés voisins) et `min_samples` (le nombre minimum de points pour former un cluster).

3. **Optimisation du seuil pour la méthode du Z-Score** :
   - Exploration de plusieurs seuils (de 3 à 1) pour identifier celui qui capture le plus grand nombre d'outliers. 


---

# Section 5 : Algorithmes Non Basés sur le Clustering
<a name="section-5"></a>

## Introduction

Dans la session précédente, nous avons exploré les algorithmes de clustering pour la détection des anomalies. Dans cette section, nous allons nous concentrer sur les algorithmes non basés sur le clustering, qui détectent les anomalies en évaluant chaque point de données individuellement ou dans des contextes localisés, sans former de clusters globaux.

## Différences entre Algorithmes de Clustering et Non Clustering

- **Algorithmes de Clustering :** Ils regroupent les points de données en clusters basés sur la similarité. Les anomalies sont identifiées comme des points qui dévient de manière significative de ces clusters. Les méthodes incluent K-means, DBSCAN, les modèles de mélanges gaussiens (GMM), et le clustering hiérarchique.

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 5 - (05-Exercice 2 - clustering)
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇



- **Algorithmes Non Clustering :** Ils évaluent chaque point de données individuellement, en se basant sur des propriétés statistiques ou de densité locales. Les anomalies sont détectées en fonction de leur déviation par rapport au comportement attendu. Les algorithmes incluent Isolation Forest, One-Class SVM, et Local Outlier Factor (LOF).

## Isolation Forest

**Isolation Forest** est un algorithme non basé sur le clustering qui isole les anomalies en partitionnant les données de manière récursive. Les anomalies sont rapidement isolées avec un nombre minimal de divisions car elles se trouvent dans des régions peu denses de l'espace des données.

### Fonctionnement de l'algorithme

1. **Partitionnement Aléatoire :** L'algorithme commence par diviser les données de manière aléatoire sur une caractéristique donnée.
2. **Isolation des Anomalies :** Les points isolés rapidement avec moins de divisions sont considérés comme des anomalies.
3. **Longueur du Chemin :** La longueur du chemin pour isoler un point est utilisée pour déterminer s'il est une anomalie. Une longueur de chemin courte indique une anomalie.

### Exemple

Prenons un petit ensemble de données avec deux caractéristiques. Isolation Forest partitionne les données et isole les points en fonction de leur position relative par rapport aux autres. Les points qui nécessitent moins de divisions pour être isolés sont considérés comme des anomalies.

### Programmation

En Python, vous pouvez utiliser `IsolationForest` de la bibliothèque `sklearn` pour implémenter cet algorithme. Voici un exemple de code :

```python
from sklearn.ensemble import IsolationForest

# Application de Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
iso_forest.fit(X)
anomalies = iso_forest.predict(X)
```


⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 6 - (06-Exercice 3 - non clustering)
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇


## Histogram-Based Outlier Score (HBOS)

**HBOS** est un algorithme efficace pour la détection des anomalies basé sur les histogrammes des distributions des caractéristiques. Il construit des histogrammes pour chaque caractéristique, et les anomalies sont identifiées en fonction de leur densité dans ces histogrammes.

### Fonctionnement de l'algorithme

1. **Construction des Histogrammes :** Pour chaque caractéristique, un histogramme est construit.
2. **Densité de Probabilité :** La densité de probabilité est calculée pour chaque point de données. Les points dans des bacs à faible fréquence sont considérés comme des anomalies.
3. **Score d'Anomalie :** Les scores sont combinés pour obtenir un score global d'anomalie pour chaque point de données.

### Programmation

Exemple de code pour appliquer HBOS à un ensemble de données en utilisant Python :

```python
from pyod.models.hbos import HBOS

# Application de HBOS
hbos = HBOS()
hbos.fit(X)
anomaly_scores = hbos.decision_function(X)
anomalies = hbos.predict(X)
```



⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 7 - (07-Exercice 4 - Detection via ISOLATION FOREST-TO-DO)
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇


- Cet exercice vise à identifier des anomalies dans un jeu de données de fabrication en utilisant des techniques de détection d'anomalies et de réduction de la dimensionalité. Le jeu de données utilisé est fourni sous le nom `IsoForestDatasetPredMain.xlsx`.

#### Description du jeu de données

Le jeu de données provient d'un scénario de fabrication et contient les informations suivantes :
- Température de l'air (K)
- Température du processus (K)
- Vitesse de rotation (rpm)
- Couple (Nm)
- Usure de l'outil (min)

#### Objectif

L'objectif de cet exercice est de programmer la détection d'anomalies dans le jeu de données à travers deux processus distincts :

1. **Détection Initiale d'Anomalies** : Utiliser l'algorithme Isolation Forest avec un taux de contamination de 5 % pour identifier le premier ensemble d'anomalies.
   
2. **Affinement des Anomalies Détectées** : Parmi les anomalies identifiées, isoler celles qui se situent dans les 90e et 95e percentiles basés sur leur distance par rapport à la moyenne du jeu de données.

#### Étapes supplémentaires

- **Réduction de la Dimensionalité** : Réduire le jeu de données à deux composantes principales à l'aide de l'Analyse en Composantes Principales (PCA).
- **Visualisation** : Visualiser les anomalies détectées en utilisant un diagramme de dispersion avec des marqueurs distincts pour les points normaux et les anomalies.
- **Représentation des Clusters et Marquage des Anomalies** : Ajouter un cercle pour représenter les clusters et des flèches pour marquer les anomalies en dehors de ces clusters.

---

### Correction de l'exercice

1. **Détection Initiale des Anomalies :** Utilisez l'algorithme Isolation Forest avec un taux de contamination de 5 % pour identifier un premier ensemble d'anomalies.



```python
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from scipy.spatial import distance
import matplotlib.pyplot as plt

# Charger le jeu de données
data = pd.read_excel('IsoForestDatasetPredMain.xlsx')

# Sélectionner les caractéristiques
features = ['Température de l\'air [K]', 'Température du processus [K]', 'Vitesse de rotation [rpm]', 'Couple [Nm]', 'Usure de l\'outil [min]']
X = data[features]

# Appliquer Isolation Forest pour la détection initiale des anomalies
iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)
iso_forest.fit(X)
data['scores'] = iso_forest.decision_function(X)
data['anomaly'] = iso_forest.predict(X)

# Filtrer les anomalies (où anomaly == -1)
anomalies = data[data['anomaly'] == -1]

# Calculer la distance Euclidienne par rapport à la moyenne pour chaque anomalie
mean_values = X.mean(axis=0)
anomalies['distance'] = anomalies[features].apply(lambda row: distance.euclidean(row, mean_values), axis=1)

# Identifier les anomalies dans les 95e et 90e percentiles de la distance
percentile_95th = np.percentile(anomalies['distance'], 95)
percentile_90th = np.percentile(anomalies['distance'], 90)

anomalies_95th = anomalies[anomalies['distance'] >= percentile_95th]
anomalies_90th = anomalies[anomalies['distance'] >= percentile_90th]

# Imprimer les enregistrements correspondant à ces percentiles
print("Anomalies dans le 95e percentile basées sur la distance :")
print(anomalies_95th[['Numéro de référence'] + features])

print("\nAnomalies dans le 90e percentile basées sur la distance :")
print(anomalies_90th[['Numéro de référence'] + features])
```

---


2. **Raffinement des Anomalies :** Parmi les anomalies identifiées, isolez celles qui se situent dans les percentiles 90 et 95, en fonction de leur distance par rapport à la moyenne de l'ensemble des données.


```python
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
import seaborn as sns

# Charger le jeu de données
data = pd.read_excel('IsoForestDatasetPredMain.xlsx')
features = ['Température de l\'air [K]', 'Température du processus [K]', 'Vitesse de rotation [rpm]', 'Couple [Nm]', 'Usure de l\'outil [min]']

# Standardiser les données
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[features])

# PCA pour la réduction de la dimensionalité à 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
data_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])

# Appliquer Isolation Forest pour la détection des anomalies
iso = IsolationForest(contamination=0.05)
data_pca['anomaly'] = iso.fit_predict(data_pca)

# Visualisation des résultats
plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='anomaly', data=data_pca, palette=['blue', 'red'], legend='full', style='anomaly', markers=['o', 'x'])
plt.title('Détection des Anomalies avec PCA et Isolation Forest')
plt.xlabel('Composante Principale 1')
plt.ylabel('Composante Principale 2')
plt.legend(title='Type de Données', labels=['Normal', 'Anomalie'])
plt.grid(True)
plt.show()
```

---



3. **Visualisation :** Réduisez la dimensionnalité des données avec PCA et visualisez les anomalies.


```python
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from matplotlib.patches import Circle

# Charger le jeu de données
data = pd.read_excel('IsoForestDatasetPredMain.xlsx')
features = ['Température de l\'air [K]', 'Température du processus [K]', 'Vitesse de rotation [rpm]', 'Couple [Nm]', 'Usure de l\'outil [min]']

# Standardiser les données
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[features])

# PCA pour la réduction de la dimensionalité à 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
data_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])

# Appliquer Isolation Forest pour la détection des anomalies
iso = IsolationForest(contamination=0.05)
data_pca['anomaly'] = iso.fit_predict(data_pca[['PC1', 'PC2']])

# Définir les données normales et les anomalies
normal_data = data_pca[data_pca['anomaly'] == 1]
anomalies = data_pca[data_pca['anomaly'] == -1]

# Tracer
fig, ax = plt.subplots(figsize=(10, 6))

# Tracer les points normaux
ax.scatter(normal_data['PC1'], normal_data['PC2'], color='blue', label='Données Normales')

# Tracer les anomalies
ax.scatter(anomalies['PC1'], anomalies['PC2'], color='red', label='Anomalies', marker='x')

# Cercle pour indiquer le cluster principal
circle_radius = np.std(normal_data[['PC1', 'PC2']].values)  # Écart-type comme rayon du cercle de cluster
circle = Circle(xy=(np.mean(normal_data['PC1']), np.mean(normal_data['PC2'])), radius=circle_radius, color='green', fill=False, linewidth=1.5, label='Limite du Cluster')
ax.add_patch(circle)

# Flèches pointant vers les anomalies
for index, anomaly in anomalies.iterrows():
    ax.annotate('', xy=(anomaly['PC1'], anomaly['PC2']), xytext=(np.mean(normal_data['PC1']), np.mean(normal_data['PC2'])),
                arrowprops=dict(facecolor='red', shrink=0.05, width=1.5, headwidth=8))

# Améliorer le graphique
ax.set_title('Données Réduites par PCA avec Indicateurs d\'Anomalie')
ax.set_xlabel('Composante Principale 1')
ax.set_ylabel('Composante Principale 2')
ax.legend()
plt.grid(True)
plt.show()
```


---

## Approche Hybride

Une approche hybride combine les avantages des algorithmes de clustering et des algorithmes non clustering pour améliorer la précision de la détection des anomalies. Par exemple, vous pouvez utiliser **GMM** pour identifier les anomalies globales et **LOF** pour détecter les anomalies locales au sein de ces clusters.

### Exemple d'Implémentation Hybride

En appliquant GMM pour créer des clusters, puis en appliquant LOF à chaque cluster, vous pouvez identifier un plus large éventail d'anomalies, y compris celles qui pourraient être ignorées par une seule méthode.

```python
from sklearn.mixture import GaussianMixture
from sklearn.neighbors import LocalOutlierFactor

# Application de GMM pour le clustering
gmm = GaussianMixture(n_components=3)
gmm_labels = gmm.fit_predict(X)

# Application de LOF à chaque cluster
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)
lof_anomalies = lof.fit_predict(X[gmm_labels == 0])
```

----

⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️⬇️
# Exercice Pratique 8 - EXERCICE (08-Exercice 5 - Hybride - clustering + non clustering)
⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️⬆️






# Référence :

- https://drive.google.com/drive/folders/1H7kbeWT_kd-vCTm3jFhZygDMAl28kdJn?usp=sharing

 ## Exemple de code pour cet exercice :

```python
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Chargement des données
data = pd.read_csv('FraudDetectionInsuranceNonClusterv1.csv')

# Détection initiale des anomalies
iso_forest = IsolationForest(contamination=0.05)
anomalies = iso_forest.fit_predict(data)

# Raffinement des anomalies
pca = PCA(n_components=2)
data_reduced = pca.fit_transform(data)
plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=anomalies)
plt.show()
```

[Retour en haut](#table-des-matières)

---

# Section 6 : Explication des Résultats de la Détection d'Anomalies dans un Contexte de Fraude
<a name="section-6"></a>

#### Contexte : Détection de Fraude dans l'Assurance Automobile

Dans cette section, nous allons approfondir la manière d'expliquer les résultats d'un algorithme de détection d'anomalies, en prenant pour exemple un scénario de détection de fraude dans le secteur de l'assurance automobile. Ce domaine est particulièrement sensible, car la détection d'anomalies y signifie potentiellement l'identification de comportements frauduleux de la part des clients, des partenaires, ou même des employés.

Le type d'analyse que nous réalisons est un apprentissage non supervisé, c'est-à-dire que nous n'avons pas de données labellisées indiquant quels cas sont des fraudes. Cela rend l'explication des résultats encore plus cruciale, car nous devons convaincre les parties prenantes que les anomalies identifiées par notre modèle sont dignes d'une investigation plus poussée.

#### Importance de l'Explication des Anomalies

Lorsque nous présentons les résultats d'un modèle de détection d'anomalies à des clients ou à la direction, la question qui revient souvent est : *"Comment pouvez-vous affirmer que ces cas sont des anomalies ?"*. Cette question est encore plus pertinente dans le cadre de la détection de fraude, car signaler une anomalie peut être perçu comme une accusation directe de malversation. Pour éviter toute incompréhension ou méfiance, il est impératif de pouvoir fournir une explication claire et détaillée des raisons pour lesquelles un point de données spécifique est considéré comme une anomalie.

Dans un contexte de détection de fraude, il est important de faire preuve de diplomatie. Par exemple, au lieu de dire directement que certaines transactions sont frauduleuses, nous pouvons les décrire comme des "cas potentiellement non conformes" qui nécessitent une enquête plus approfondie. Toutefois, même en adoucissant le message, l'essentiel reste que nous pointons du doigt des transactions suspectes, ce qui implique que certaines personnes ou organisations pourraient être suspectées de fraude.

#### Utilité des Algorithmes d'Explication : SHAP

Expliquer pourquoi un algorithme détecte certaines anomalies est essentiel, surtout dans un contexte où les résultats du modèle peuvent avoir des conséquences significatives sur les opérations d'une entreprise. C'est là qu'intervient SHAP (SHapley Additive exPlanations), un algorithme d'explication qui permet de décomposer les décisions de modèles complexes, comme les modèles de forêts d'isolement (Isolation Forest), pour montrer comment chaque facteur individuel a contribué à l'identification d'une anomalie.

Dans notre scénario, nous analysons 32 facteurs différents pour identifier des anomalies dans les données. Le but de SHAP est de nous aider à isoler les facteurs critiques qui ont conduit à l'identification de ces anomalies. Ce type d'information est extrêmement précieux non seulement pour justifier pourquoi certaines transactions sont suspectes, mais aussi pour aider à mettre en place des contrôles préventifs visant à minimiser les risques de fraude à l'avenir.

#### Construction du Modèle : Isolation Forest

La première étape consiste à construire un modèle Isolation Forest pour identifier les anomalies dans notre jeu de données. Isolation Forest est un algorithme qui fonctionne en isolant les anomalies en se basant sur la distance qui les sépare des autres points de données. Plus un point est isolé rapidement, plus il est susceptible d'être une anomalie.

**Étape 1 : Importation des Bibliothèques et Chargement des Données**

Nous commençons par importer les bibliothèques nécessaires, y compris SHAP pour l'explication des anomalies, et pandas pour la manipulation des données.

```python
import shap
import pandas as pd
from sklearn.ensemble import IsolationForest

# Chargement du jeu de données
data = pd.read_csv('FraudDetectionInsuranceNonClusterv1.csv')
```

**Étape 2 : Construction du Modèle Isolation Forest**

Ensuite, nous construisons le modèle Isolation Forest en définissant le paramètre de contamination à 1 %. Cela signifie que nous demandons à l'algorithme de détecter au moins 1 % des points de données comme des anomalies. Le modèle est ensuite ajusté (fit) aux données.

```python
# Construction du modèle Isolation Forest
iso_forest = IsolationForest(contamination=0.01, random_state=42)
iso_forest.fit(data)
```

#### Explication des Anomalies avec SHAP

Une fois le modèle Isolation Forest en place, nous utilisons SHAP pour expliquer pourquoi certains points de données ont été identifiés comme des anomalies. SHAP attribue une valeur à chaque facteur, ce qui permet de voir l'importance relative de chaque facteur dans la décision d'isoler un point de données comme anomalie.

**Étape 3 : Utilisation de SHAP pour Expliquer les Anomalies**

SHAP nous permet d'analyser chaque point d'anomalie individuellement et de comprendre quels facteurs spécifiques ont contribué à son identification.

```python
# Utilisation de SHAP pour expliquer les anomalies
explainer = shap.Explainer(iso_forest, data)
shap_values = explainer(data)

# Boucle sur chaque anomalie pour afficher les facteurs contributifs
for i in range(len(data)):
    shap.force_plot(explainer.expected_value, shap_values[i], data.iloc[i])
```

- Dans cette analyse, les facteurs marqués en rouge sont ceux qui ont un impact positif significatif sur la détection de l'anomalie, tandis que ceux en bleu ont un impact négatif ou négligeable. Par exemple, si un point de données présente une anomalie due à une combinaison spécifique de type de collecte et de fabrication automatique, ces facteurs apparaîtront en rouge, signalant leur importance.

#### Identification des Facteurs Critiques à l'Échelle Globale

Au-delà de l'explication des anomalies individuelles, SHAP peut également être utilisé pour identifier les principaux facteurs contribuant aux anomalies dans l'ensemble du jeu de données. Cela est essentiel pour les équipes opérationnelles, qui peuvent utiliser ces informations pour renforcer les contrôles internes et prévenir la fraude à l'avenir.

**Étape 4 : Détermination des Facteurs Clés de l'Ensemble des Anomalies**

Pour identifier les principaux facteurs influençant les anomalies, nous calculons la valeur absolue moyenne des contributions SHAP pour chaque facteur, que nous classons ensuite par ordre décroissant.

```python
# Identification des facteurs critiques à l'échelle globale
mean_shap_values = np.abs(shap_values.values).mean(axis=0)
important_factors = pd.DataFrame(list(zip(data.columns, mean_shap_values)), columns=['Facteur', 'Importance SHAP'])
important_factors = important_factors.sort_values(by='Importance SHAP', ascending=False)

# Affichage des principaux facteurs
print(important_factors.head(5))
```

En classant les facteurs en fonction de leur importance SHAP, nous pouvons voir quels facteurs, tels que la limite d'assurance, l'âge du client ou la gravité de l'incident, ont le plus grand impact sur la détection des anomalies.

#### Application Pratique : Contrôles Préventifs Basés sur SHAP

- Les informations fournies par SHAP ne servent pas seulement à expliquer les résultats du modèle, mais aussi à élaborer des stratégies préventives

.
- Par exemple, si l'analyse montre que la limite d'assurance et l'âge du client sont des facteurs critiques pour la fraude, des contrôles spécifiques peuvent être mis en place pour surveiller de plus près ces variables lors de l'évaluation des nouvelles demandes d'assurance.

### Conclusion de la partie 6

- L'utilisation d'algorithmes d'explication comme SHAP en conjonction avec des modèles de détection d'anomalies tels qu'Isolation Forest permet non seulement d'identifier des anomalies, mais aussi de comprendre pourquoi ces anomalies se produisent.
- Cette compréhension approfondie est essentielle non seulement pour justifier les décisions de l'algorithme, mais aussi pour mettre en place des mesures de prévention efficaces et ainsi réduire les risques de fraude à l'avenir.

[Retour en haut](#table-des-matières)

---

# Section 7 : Comparaison des Algorithmes de Détection d'Anomalies avec la Bibliothèque PyOD
<a name="section-7"></a>

#### Introduction à PyOD et Contexte de l'Exercice

- Dans cette section, nous allons explorer l'utilisation de la bibliothèque **PyOD** (Python Outlier Detection), un outil puissant pour la détection d'anomalies dans les données multivariées.
- PyOD est un ensemble de plus de 30 algorithmes différents, qui ont été largement utilisés dans la recherche académique pour détecter des anomalies dans divers types de jeux de données.

- Pour illustrer l'efficacité de PyOD, nous appliquerons plusieurs de ces algorithmes sur un jeu de données provenant de Kaggle, spécifiquement un jeu de données sur les assurances santé.
- Ce jeu de données est utilisé pour prédire la persistance des clients à payer leurs primes d'assurance. Dans ce contexte, un client qui ne paie pas sa prime est considéré comme une anomalie.

- Le jeu de données contient environ 80 000 points de données et 13 attributs différents, dont 9 attributs continus et 2 attributs nominaux.
- Ces attributs incluent des informations telles que l'âge, le montant de la prime, le revenu, la région, le canal de souscription, et d'autres variables pertinentes pour prédire la persistance des paiements.

#### Application des Algorithmes de Détection d'Anomalies

Pour cette analyse, nous avons sélectionné 10 algorithmes de détection d'anomalies différents, couvrant une variété de méthodes, notamment :

- **Algorithmes basés sur la proximité** : Score d'anomalie basé sur la distance, détection basée sur les k plus proches voisins (k-NN), etc.
- **Algorithmes basés sur les forêts** : Isolation Forest, un algorithme populaire pour la détection d'anomalies.
- **Algorithmes basés sur les composantes principales** : Analyse en Composantes Principales (PCA).
- **Réseaux de neurones** : Bien que généralement plus adaptés aux données de type image ou vidéo, nous appliquons également des modèles de deep learning pour observer leur performance sur des données tabulaires.

Après avoir appliqué ces algorithmes, nous évaluerons leur performance en termes de précision, en comparant la détection correcte des anomalies avec les connaissances préalables que nous avons sur les clients qui n'ont pas payé leurs primes.

#### Précision et Évaluation des Modèles

Il est crucial de noter que l'évaluation de la précision globale d'un modèle peut être trompeuse, surtout dans un contexte de détection d'anomalies. Il est plus pertinent d'évaluer la précision séparément pour les anomalies et les non-anomalies, car les actions que nous entreprenons en fonction de ces classifications peuvent avoir des impacts significatifs.

Par exemple, si un client est incorrectement identifié comme une anomalie (c'est-à-dire qu'il est identifié à tort comme un client qui ne paiera pas), cela pourrait conduire à des décisions commerciales erronées, comme la résiliation prématurée d'une police d'assurance ou des actions judiciaires inutiles. Inversement, ne pas identifier une véritable anomalie pourrait laisser passer une fraude ou un risque élevé.

#### Implémentation du Code et Résultats

Voici une implémentation détaillée pour comparer différents algorithmes de détection d'anomalies en utilisant PyOD :

1. **Prétraitement des Données :**
   - Remplacement des valeurs manquantes par la moyenne (pour les valeurs numériques) ou par la modalité (pour les valeurs catégorielles).
   - Encodage des variables nominales pour les rendre compatibles avec les algorithmes.
   - Normalisation des données pour traiter les outliers et préparer les données pour la modélisation.

2. **Application des Algorithmes :**
   - Importation des algorithmes disponibles dans PyOD.
   - Création d'un dictionnaire pour stocker les modèles et les résultats.
   - Application de chaque algorithme sur le jeu de données traité.
   - Enregistrement des résultats de chaque modèle, incluant la durée d'exécution et la proportion d'anomalies détectées.

3. **Analyse des Résultats :**
   - Comparaison de la précision des modèles, à la fois pour les anomalies et les non-anomalies.
   - Visualisation des résultats pour identifier quel algorithme offre la meilleure performance pour ce jeu de données spécifique.

## Extrait de code pour illustrer l'application de PyOD :

```python
# Importation des bibliothèques nécessaires
import numpy as np
import pandas as pd
from pyod.models.knn import KNN
from pyod.models.iforest import IForest
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

# Chargement et prétraitement des données
data = pd.read_csv('chemin_vers_votre_fichier.csv')
data.fillna(data.mean(), inplace=True)
encoder = LabelEncoder()
data['sourcing_channel'] = encoder.fit_transform(data['sourcing_channel'])
data['residence_area_type'] = encoder.fit_transform(data['residence_area_type'])
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)

# Application de l'algorithme Isolation Forest
model = IForest(contamination=0.06)
model.fit(data_scaled)
data['anomaly'] = model.labels_

# Évaluation des résultats
print("Nombre d'anomalies détectées : ", sum(data['anomaly'] == 1))
```

#### Conclusion et Recommandations

PyOD est une bibliothèque puissante pour explorer et comparer différents algorithmes de détection d'anomalies. Cependant, il est recommandé de commencer par tester ces algorithmes sur des jeux de données expérimentaux avant de les déployer dans un contexte commercial, afin d'ajuster les paramètres et de s'assurer que les modèles offrent une précision suffisante pour les anomalies et les non-anomalies.

En comprenant les forces et les faiblesses de chaque algorithme dans différents contextes, les praticiens peuvent mieux sélectionner l'outil adapté à leurs besoins spécifiques en matière de détection d'anomalies, minimisant ainsi les risques de décisions incorrectes et améliorant la gestion proactive des fraudes et autres irrégularités.

[Retour en haut](#table-des-matières)



---


# Annexe 1 :  Comprendre les Anomalies et les Outliers ?

## Introduction

Dans le cadre de l'analyse de données, il est essentiel de comprendre les concepts d'anomalies et d'outliers. Ces notions sont fondamentales pour des applications telles que la détection de fraude, la maintenance prédictive, et l'amélioration de la qualité des données. Ce document clarifie la distinction entre anomalies et outliers, et explique leur importance dans l'analyse de données.

## Définition des Concepts

### Anomalie

Une **anomalie** est un point de données ou un ensemble de points qui se distingue par un comportement inhabituel ou inattendu par rapport au reste du jeu de données. Les anomalies peuvent signaler des événements rares, des erreurs, ou des comportements qui ne suivent pas la tendance générale.

#### Types d'anomalies :
1. **Anomalies basées sur le temps** : Les points de données sont dépendants du temps. Par exemple, les variations du prix de l'essence sur une période.
2. **Anomalies non basées sur le temps** : Les points de données ne dépendent pas du temps. Par exemple, le prix d'un appartement en fonction de divers facteurs.
3. **Anomalies d'image** : Anomalies détectées dans des ensembles d'images, souvent utilisées dans la vision par ordinateur.

### Outlier

Un **outlier** est un point de données qui se situe loin de la majorité des autres points dans un jeu de données. Il est souvent identifié statistiquement par sa distance par rapport à d'autres points.

### Relation entre Outliers et Anomalies

- **Dans certains cas** : Chaque outlier est considéré comme une anomalie. Cette approche simplifie l'analyse et aide à identifier rapidement des points de données potentiellement problématiques. Par exemple, dans un **datawarehouse bancaire**, la détection de transactions financières est très stricte. Tout montant qui s'écarte significativement des transactions habituelles d'un client peut être considéré comme une anomalie pour prévenir les fraudes ou identifier des erreurs. De même, dans les systèmes de **prévention des fraudes par carte de crédit**, chaque transaction considérée comme un outlier par rapport aux habitudes d'achat d'un client (comme un montant inhabituel ou une localisation géographique étrange) est automatiquement flaguée comme une anomalie et nécessitera une vérification supplémentaire.
- **Dans un contexte plus large** : Un outlier n'est pas nécessairement une anomalie. Par exemple, une transaction exceptionnellement élevée peut être normale pour certains clients et ne serait donc pas considérée comme une anomalie.

### Valeurs Nulles

Une **valeur nulle** représente l'absence de données. Bien qu'une valeur nulle ne soit pas automatiquement une anomalie, elle peut en indiquer une si elle apparaît dans un contexte où aucune donnée ne devrait être manquante. Par exemple, un champ obligatoire laissé vide dans une base de données pourrait être interprété comme une anomalie.

## Méthodes de Détection des Anomalies

### Algorithmes Supervisés

Les algorithmes supervisés sont utilisés lorsque des exemples d'anomalies passées sont disponibles. Ils permettent de classifier de nouveaux points de données en fonction de ces références.

### Algorithmes Non Supervisés

Les algorithmes non supervisés sont utilisés lorsque aucune référence passée n'est disponible. Ils sont souvent préférés en entreprise car les données passées ne sont pas toujours fiables ou disponibles. Ces algorithmes, basés sur des clusters ou d'autres méthodes, détectent des comportements inattendus sans connaissance préalable.

## Exemples Pratiques

1. **Jeux de données basés sur le temps** : Transactions bancaires à différents moments.
2. **Jeux de données non basés sur le temps** : Prix des appartements en fonction de divers facteurs.

La détection d'anomalies dans les jeux de données basés sur le temps est particulièrement utile pour identifier des événements rares mais critiques, comme la maintenance prédictive dans l'industrie.

### Exemples concrets où les outliers sont toujours des anomalies

1. **Datawarehouse bancaire** : Dans une banque, les transactions financières sont rigoureusement surveillées. Si une transaction présente un montant qui dévie fortement des habitudes d'un client, elle est automatiquement considérée comme une anomalie. Cette approche stricte permet de détecter rapidement les activités frauduleuses ou les erreurs de saisie.
  
2. **Prévention des fraudes par carte de crédit** : Les systèmes de détection de fraude considèrent chaque outlier comme une anomalie potentielle. Par exemple, si un client qui effectue habituellement de petites transactions locales réalise soudainement un achat important à l'étranger, cela sera marqué comme une anomalie, et une alerte sera déclenchée.

3. **Analyse des risques d'assurance** : Les compagnies d'assurance utilisent des modèles où chaque outlier dans les réclamations (comme un montant de sinistre très élevé ou une fréquence de réclamations inhabituelle) est immédiatement considéré comme une anomalie. Cela permet de réduire les risques de fraude ou de fausses déclarations.

## Conclusion

Dans l'analyse de données, bien comprendre la différence entre anomalies et outliers est crucial. Si, dans certains cas, chaque outlier est considéré comme une anomalie, cette règle peut ne pas s'appliquer dans toutes les situations. Il est important de choisir les bons outils et méthodes pour détecter efficacement ces phénomènes, en tenant compte des spécificités de votre jeu de données et du contexte de l'analyse.

---



# Annexe 2 :  Comprendre les Anomalies et les Outliers ?


**Dans certains cas, par exemple ou le contexte est stricte** : Chaque outlier est considéré comme une anomalie. Cette approche simplifie l'analyse et aide à identifier rapidement des points de données potentiellement problématiques. 

### Exemples de contextes stricts :

1. **Datawarehouse bancaire** : Dans un environnement bancaire, la surveillance des transactions financières est extrêmement rigoureuse. Toute transaction qui dévie significativement des habitudes d'un client, que ce soit par son montant ou sa localisation, est automatiquement considérée comme une anomalie. Par exemple, si un client effectue habituellement des transactions de faible montant et qu'une transaction soudainement élevée apparaît, cela est immédiatement flagué pour une enquête approfondie, afin de prévenir les fraudes ou de corriger des erreurs potentielles.

2. **Prévention des fraudes par carte de crédit** : Dans les systèmes de détection de fraude par carte de crédit, chaque transaction identifiée comme un outlier est immédiatement traitée comme une anomalie. Par exemple, un achat important réalisé dans un pays où le client n'a jamais voyagé ou une série de petites transactions effectuées en peu de temps dans différents endroits peut indiquer une activité frauduleuse. Ces anomalies déclenchent généralement des alertes automatiques et peuvent entraîner la suspension de la carte jusqu'à ce que la situation soit clarifiée.

3. **Signes vitaux en médecine** : Dans le domaine médical, les signes vitaux des patients (tels que la fréquence cardiaque, la tension artérielle, la température corporelle, etc.) sont surveillés en temps réel. Chaque outlier est traité comme une anomalie potentiellement critique. Par exemple, une chute soudaine de la pression artérielle ou une augmentation rapide de la température corporelle par rapport aux valeurs normales du patient est immédiatement considérée comme une anomalie. Cette détection précoce permet aux professionnels de la santé de réagir rapidement pour éviter des complications graves ou des urgences médicales.

4. **Surveillance industrielle et maintenance prédictive** : Dans les environnements industriels, les capteurs surveillent en continu les performances des machines. Si un capteur enregistre une température ou une vibration qui dévie significativement des valeurs normales, cela est traité comme une anomalie. Par exemple, une machine qui fonctionne habituellement à une certaine température pourrait indiquer un problème mécanique si la température dépasse soudainement ce seuil. L'identification de ces anomalies permet une intervention rapide, prévenant ainsi les pannes coûteuses et les arrêts de production non planifiés.

---

# Annexe 3 :  La Fin du Mythe de la Black Box en Machine Learning : Mythe ou Réalité ?

Dans le monde du machine learning, la notion de "boîte noire" a toujours été un sujet de préoccupation. L'idée est simple : les modèles complexes, tels que les réseaux de neurones profonds, prennent des décisions que même leurs créateurs peuvent avoir du mal à comprendre ou à expliquer. Cette opacité a conduit à des critiques sur la confiance que l'on peut accorder à ces modèles, surtout dans des domaines critiques comme la santé, la finance ou la justice.

Cependant, avec l'émergence d'outils d'explicabilité comme SHAP (SHapley Additive exPlanations), certains affirment que nous avons résolu le problème de la boîte noire. Mais est-ce vraiment le cas ? La fin de la boîte noire est-elle une réalité ou un mythe ?

#### SHAP : Un Pas en Avant Vers l'Explicabilité

SHAP est une méthode basée sur la théorie des jeux qui attribue une valeur de Shapley à chaque caractéristique d'un modèle, indiquant l'importance de cette caractéristique pour une prédiction donnée. Ce mécanisme a révolutionné la manière dont nous pouvons interpréter les modèles complexes, permettant de fournir des explications précises et cohérentes sur les décisions des modèles.

Avec SHAP, nous pouvons maintenant répondre à des questions comme "Pourquoi le modèle a-t-il prédit que cet utilisateur ne rembourserait pas son prêt ?" en identifiant les caractéristiques spécifiques qui ont conduit à cette prédiction. Cela représente un progrès significatif par rapport à la situation précédente, où les modèles de réseaux de neurones étaient souvent considérés comme des boîtes noires impenetrables.

#### L'Explicabilité : Une Solution Complète ?

Cependant, il est essentiel de nuancer cette avancée. Bien que SHAP et d'autres techniques d'explicabilité réduisent l'opacité des modèles, ils ne rendent pas ces modèles totalement transparents ou interprétables dans tous les sens.

1. **Explicabilité vs. Interprétabilité** : SHAP nous donne des explications sur les décisions des modèles, mais il ne rend pas les modèles eux-mêmes plus simples ou plus transparents. Un réseau de neurones reste un modèle complexe avec de nombreuses couches et des interactions non linéaires difficiles à comprendre dans leur intégralité.

2. **Complexité des Explications** : Les valeurs de Shapley sont utiles, mais elles peuvent être difficiles à interpréter, surtout pour les modèles avec des milliers de caractéristiques. Comprendre l'importance relative de chaque caractéristique et comment elles interagissent peut rapidement devenir un casse-tête.

3. **Approximation et Fiabilité** : Les explications fournies par SHAP sont des approximations basées sur des modèles complexes. Ces approximations sont souvent très précises, mais elles ne sont pas parfaites. Dans certains cas, elles peuvent ne pas capturer toutes les subtilités du modèle.

4. **Coût Computationnel** : Utiliser SHAP pour expliquer un modèle complexe peut être très coûteux en termes de temps de calcul et de ressources. Pour des modèles très grands ou des ensembles de données volumineux, cela peut poser des problèmes pratiques.

#### La Boîte Noire : Un Problème Révolu ?

Dire que le problème de la boîte noire est "révolu" serait une exagération. Nous avons fait des progrès énormes grâce à des outils comme SHAP, mais la complexité inhérente des modèles modernes signifie qu'il reste encore des défis à relever. 

En réalité, l'explicabilité n'est qu'une partie de la solution. Il est également crucial de travailler sur la transparence des modèles, l'éthique des décisions automatisées, et l'interprétabilité globale pour garantir que nous pouvons vraiment comprendre et faire confiance aux systèmes d'IA que nous développons.

#### Conclusion

L'affirmation selon laquelle le problème de la boîte noire est résolu est un mythe partiellement vrai. Oui, nous avons des outils comme SHAP qui nous permettent de mieux comprendre les modèles complexes, mais cela ne signifie pas que tous les défis liés à l'opacité des modèles sont résolus. L'explicabilité est un pas en avant, mais la route vers une compréhension complète et une transparence totale est encore longue. Il est important de continuer à chercher des solutions qui allient puissance de prédiction et clarté pour que la confiance dans les modèles de machine learning puisse véritablement se développer.

L'histoire de la boîte noire n'est pas encore terminée, mais avec les avancées actuelles, nous sommes certainement en train d'écrire son prochain chapitre.



